{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Tune -Ray Tune with Sklearn Hyperparameter Tuning\n",
    "\n",
    "© 2019-2021, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../images/AnyscaleAcademyLogo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Tune's Scikit Learn Drop-in Replacements\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/tune-sklearn1.png\" align=\"center\" width=\"50%\">\n",
    "\n",
    "Scikit-Learn is one of the most widely used tools in the ML community for working with data, offering dozens of easy-to-use machine learning algorithms. However, to achieve high performance for these algorithms, you often need to perform **model selection**. Model selection is way to get the best performant model, after tuning over a set of parameters.\n",
    "\n",
    "`tune-sklearn` is a module that integrates Ray Tune's hyperparameter tuning and scikit-learn's Classifier API. `tune-sklearn` has two APIs: [TuneSearchCV](https://docs.ray.io/en/latest/tune/api_docs/sklearn.html#tunesearchcv-docs), and [TuneGridSearchCV](https://docs.ray.io/en/latest/tune/api_docs/sklearn.html#tunesearchcv-docs). They are drop-in replacements for Scikit-learn's [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) and [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=gridsearchcv#sklearn.model_selection.GridSearchCV), so you only need to change less than five lines in a standard Scikit-Learn script to use the API.\n",
    "\n",
    "Let's compare Tune's Scikit-Learn APIs to the standard scikit-learn GridSearchCV. For this example, we'll be using `TuneGridSearchCV` with a stochastic gradient descent (SGD) [SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html).\n",
    "\n",
    "To start out, change the import statement to get tune-scikit-learn’s grid search cross validation interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Import Tune's replacements\n",
    "from ray.tune.sklearn import TuneGridSearchCV\n",
    "from ray.tune.sklearn import TuneSearchCV\n",
    "from ray.tune.logger import LoggerCallback\n",
    "\n",
    "# Other relevant imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use the stochastic gradient descent (SGD) classifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# import the classification dataset\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create classification data using `sklearn.datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classification_data() -> (np.ndarray, np.ndarray):\n",
    "    X, y = make_classification(\n",
    "        n_samples=11000,\n",
    "        n_features=1000,\n",
    "        n_informative=50,\n",
    "        n_redundant=0,\n",
    "        n_classes=10,\n",
    "        class_sep=2.5)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the classifcation data, training and test data sets, and define our hyperparameter\n",
    "grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_classification_data()\n",
    "# Split the dataset into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=1000)\n",
    "\n",
    "# Example parameters grid to tune from SGDClassifier\n",
    "parameter_grid = {\"alpha\": [1e-4, 1e-1, 1], \"epsilon\": [0.01, 0.1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Sklearn to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# n_jobs=-1 enables use of all cores does\n",
    "sklearn_search = GridSearchCV(SGDClassifier(), parameter_grid, n_jobs=-1, verbose=True)\n",
    "sklearn_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters found were: \", sklearn_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Ray's Tune's drop-in replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from there, we proceed just like how we would in Scikit-Learn’s interface!\n",
    "\n",
    "The `SGDClassifier` has a `partial_fit` API, which enables it to stop fitting to the data for a certain hyperparameter configuration. If the estimator does not support early stopping, we would fall back to a parallel grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the setup here is exactly how you would do it for Scikit-Learn. Now, let's try fitting a model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Ray on the local host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the slight differences we introduced below:\n",
    "\n",
    " * a `early_stopping`, and\n",
    " * a specification of `max_iters` parameter\n",
    "\n",
    "The ``early_stopping`` parameter allows us to terminate unpromising configurations. If ``early_stopping=True``, TuneGridSearchCV will default to using Tune's [ASHAScheduler](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#tune-scheduler-hyperband). You can pass in a custom algorithm - see Tune's documentation on [schedulers](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#tune-schedulers) for a full list to choose from.\n",
    "\n",
    "``max_iters`` is the maximum number of iterations a given hyperparameter set could run for; it may run for fewer iterations if it is early stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tune_search = TuneGridSearchCV(\n",
    "    SGDClassifier(), parameter_grid, early_stopping=True, \n",
    "    max_iters=10, name=\"AcademyTraining\", verbose=1)\n",
    "tune_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters found were: \", tune_search.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using Bayesian Optimization\n",
    "\n",
    "In addition to the grid search interface, tune-sklearn also provides an interface, `TuneSearchCV`, for sampling from **distributions of hyperparameters**.\n",
    "\n",
    "In addition, you can easily enable Bayesian optimization over the distributions in only 2 lines of code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "digits = datasets.load_digits()\n",
    "x = digits.data\n",
    "y = digits.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2)\n",
    "\n",
    "clf = SGDClassifier()\n",
    "parameter_grid = {\"alpha\": (1e-4, 1), \"epsilon\": (0.01, 0.1)}\n",
    "\n",
    "bayopt_tune_search = TuneSearchCV(\n",
    "    clf,\n",
    "    parameter_grid,\n",
    "    search_optimization=\"bayesian\",\n",
    "    n_trials=3,\n",
    "    early_stopping=True,\n",
    "    max_iters=10,\n",
    "    verbose=1,\n",
    ")\n",
    "bayopt_tune_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters found were: \", bayopt_tune_search.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
