{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Tune -Ray Tune with Sklearn Hyperparameter Tuning\n",
    "\n",
    "© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../images/AnyscaleAcademyLogo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Tune's Scikit Learn Drop-in Replacements\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/tune-sklearn1.png\" align=\"center\" width=\"50%\">\n",
    "\n",
    "Scikit-Learn is one of the most widely used tools in the ML community for working with data, offering dozens of easy-to-use machine learning algorithms. However, to achieve high performance for these algorithms, you often need to perform **model selection**. Model selection is way to elect the best performant model, after tuning over a set of parameters.\n",
    "\n",
    "`tune-sklearn` is a module that integrates Ray Tune's hyperparameter tuning and scikit-learn's Classifier API. `tune-sklearn` has two APIs: [TuneSearchCV](https://docs.ray.io/en/latest/tune/api_docs/sklearn.html#tunesearchcv-docs) and [TuneGridSearchCV](https://docs.ray.io/en/latest/tune/api_docs/sklearn.html#tunesearchcv-docs). They are drop-in replacements for scikit-learn's [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) and [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=gridsearchcv#sklearn.model_selection.GridSearchCV), so you only need to change less than five lines in a standard scikit-Learn script to use Tune's replacement API.\n",
    "\n",
    "Let's compare Tune's scikit-learn APIs to the standard scikit-learn `GridSearchCV`. For this example, we'll be using `TuneGridSearchCV` with a stochastic gradient descent (SGD) [SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html).\n",
    "\n",
    "To start out, include the import statement to get tune-scikit-learn’s grid search cross validation interface.\n",
    "\n",
    "We need to install a few libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tune-sklearn in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (0.4.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from tune-sklearn) (1.0.2)\n",
      "Requirement already satisfied: scipy in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from tune-sklearn) (1.7.2)\n",
      "Requirement already satisfied: ray[tune] in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from tune-sklearn) (1.10.0)\n",
      "Requirement already satisfied: numpy>=1.16 in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from tune-sklearn) (1.21.4)\n",
      "Requirement already satisfied: attrs in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from ray[tune]->tune-sklearn) (21.2.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from ray[tune]->tune-sklearn) (6.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from ray[tune]->tune-sklearn) (1.0.3)\n",
      "Requirement already satisfied: redis>=3.5.0 in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from ray[tune]->tune-sklearn) (4.0.2)\n",
      "Requirement already satisfied: jsonschema in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from ray[tune]->tune-sklearn) (4.2.1)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from ray[tune]->tune-sklearn) (8.0.3)\n",
      "Requirement already satisfied: filelock in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from ray[tune]->tune-sklearn) (3.4.0)\n",
      "Requirement already satisfied: protobuf>=3.15.3 in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from ray[tune]->tune-sklearn) (3.19.1)\n",
      "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from ray[tune]->tune-sklearn) (1.42.0)\n",
      "Requirement already satisfied: pandas in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from ray[tune]->tune-sklearn) (1.3.4)\n",
      "Requirement already satisfied: tabulate in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from ray[tune]->tune-sklearn) (0.8.9)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from ray[tune]->tune-sklearn) (2.4.1)\n",
      "Requirement already satisfied: requests in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from ray[tune]->tune-sklearn) (2.26.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from scikit-learn->tune-sklearn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from scikit-learn->tune-sklearn) (3.0.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from click>=7.0->ray[tune]->tune-sklearn) (4.8.2)\n",
      "Requirement already satisfied: six>=1.5.2 in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from grpcio>=1.28.1->ray[tune]->tune-sklearn) (1.16.0)\n",
      "Requirement already satisfied: deprecated in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from redis>=3.5.0->ray[tune]->tune-sklearn) (1.2.13)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from jsonschema->ray[tune]->tune-sklearn) (0.18.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from jsonschema->ray[tune]->tune-sklearn) (5.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from pandas->ray[tune]->tune-sklearn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from pandas->ray[tune]->tune-sklearn) (2021.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from requests->ray[tune]->tune-sklearn) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from requests->ray[tune]->tune-sklearn) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from requests->ray[tune]->tune-sklearn) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from requests->ray[tune]->tune-sklearn) (3.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from importlib-resources>=1.4.0->jsonschema->ray[tune]->tune-sklearn) (3.6.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from deprecated->redis>=3.5.0->ray[tune]->tune-sklearn) (1.13.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from importlib-metadata->click>=7.0->ray[tune]->tune-sklearn) (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tune-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Import Tune's replacements\n",
    "from ray.tune.sklearn import TuneGridSearchCV\n",
    "from ray.tune.sklearn import TuneSearchCV\n",
    "\n",
    "# Other relevant imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use the stochastic gradient descent (SGD) classifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# import the classification dataset\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create classification data using `sklearn.datasets`. To start with, with we using a small dataset of 11K rows and 1k columns. As an excercise you can increase the number and see the difference between using regular scikit-learn and tune-scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classification_data() -> (np.ndarray, np.ndarray):\n",
    "    X, y = make_classification(\n",
    "        n_samples=11000,\n",
    "        n_features=1000,\n",
    "        n_informative=50,\n",
    "        n_redundant=0,\n",
    "        n_classes=10,\n",
    "        class_sep=2.5)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the classifcation data, training and test data sets, and define our hyperparameter\n",
    "grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_classification_data()\n",
    "# Split the dataset into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=1000)\n",
    "\n",
    "# Example parameters grid to tune from SGDClassifier\n",
    "parameter_grid = {\"alpha\": [1e-4, 1e-1, 1], \"epsilon\": [0.01, 0.1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Sklearn to train the model\n",
    "\n",
    "Run this on a single node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "CPU times: user 2.44 s, sys: 34.8 ms, total: 2.48 s\n",
      "Wall time: 37.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SGDClassifier(), n_jobs=-1,\n",
       "             param_grid={'alpha': [0.0001, 0.1, 1], 'epsilon': [0.01, 0.1]},\n",
       "             verbose=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# n_jobs=-1 enables use of all cores\n",
    "sklearn_search = GridSearchCV(SGDClassifier(), parameter_grid, n_jobs=-1, verbose=True)\n",
    "sklearn_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found were:  {'alpha': 0.1, 'epsilon': 0.01}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best hyperparameters found were: \", sklearn_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Ray's Tune's drop-in replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from here, we proceed just like how we would in scikit-learn’s interface!\n",
    "\n",
    "The `SGDClassifier` has a `partial_fit` API, which enables it to stop fitting to the data for a certain hyperparameter configuration. If the estimator does not support early stopping, we would fall back to a parallel grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the setup here is exactly how you would do it for scikit-learn, except we replace `GridSearchCV` with `TuneGridSearchCV`. Now, let's try fitting a model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Ray on the local host\n",
    "\n",
    "This will start Ray on the localhost. If you have a cluster, then you can supply the arguments to `ray.init(...)`.\n",
    "Check the [documentation](https://docs.ray.io/en/latest/package-ref.html?highlight=ray.init#ray-init) for the specific arguments. Some examples:\n",
    " * `ray.init()`: Start Ray locally and all the relevant processes\n",
    " * `ray.init(address=\"localhost:6379\")`: connect to the localhost cluster at a specified port (for the head node)\n",
    " * `ray.init(address=\"ray://123.45.67.89:10001\")`: connect to an existing remote cluster, using the URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 11:50:25,249\tINFO services.py:1376 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '127.0.0.1',\n",
       " 'raylet_ip_address': '127.0.0.1',\n",
       " 'redis_address': '127.0.0.1:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2022-02-21_11-50-22_666292_13984/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-02-21_11-50-22_666292_13984/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2022-02-21_11-50-22_666292_13984',\n",
       " 'metrics_export_port': 61671,\n",
       " 'gcs_address': '127.0.0.1:55466',\n",
       " 'node_id': 'cb1588aed40f21144a00ee93fbf8bcd233f02bca3be664132a57e399'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the slight differences we introduced below:\n",
    "\n",
    " * an `early_stopping`, and\n",
    " * a specification of `max_iters` parameter\n",
    "\n",
    "The ``early_stopping`` parameter allows us to terminate unpromising configurations. If ``early_stopping=True``, ``TuneGridSearchCV`` will default to using Tune's [ASHAScheduler](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#tune-scheduler-hyperband). You can pass in a custom algorithm - see Tune's documentation on [schedulers](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#tune-schedulers) for a full list to choose from.\n",
    "\n",
    "``max_iters`` is the maximum number of iterations a given hyperparameter set could run for; it may run for fewer iterations if it is early stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-21 11:51:04 (running for 00:00:14.49)<br>Memory usage on this node: 17.9/32.0 GiB<br>Using AsyncHyperBand: num_stopped=3\n",
       "Bracket: Iter 4.000: 0.88255 | Iter 1.000: 0.868925<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/9.33 GiB heap, 0.0/4.66 GiB objects<br>Current best trial: 915d8_00004 with average_test_score=0.8844 and parameters={'early_stopping': True, 'early_stop_type': <EarlyStopping.PARTIAL_FIT: 1>, 'X_id': ObjectRef(ffffffffffffffffffffffffffffffffffffffff0100000001000000), 'y_id': ObjectRef(ffffffffffffffffffffffffffffffffffffffff0100000002000000), 'groups': None, 'cv': StratifiedKFold(n_splits=5, random_state=None, shuffle=False), 'fit_params': {}, 'scoring': {'score': <function _passthrough_scorer at 0x7fe4105d2050>}, 'max_iters': 10, 'return_train_score': False, 'n_jobs': 1, 'metric_name': 'average_test_score', 'alpha': 0.1, 'epsilon': 0.1, 'estimator_ids': [ObjectRef(ffffffffffffffffffffffffffffffffffffffff0100000003000000), ObjectRef(ffffffffffffffffffffffffffffffffffffffff0100000004000000), ObjectRef(ffffffffffffffffffffffffffffffffffffffff0100000005000000), ObjectRef(ffffffffffffffffffffffffffffffffffffffff0100000006000000), ObjectRef(ffffffffffffffffffffffffffffffffffffffff0100000007000000)]}<br>Result logdir: /Users/jules/ray_results/AcademyTraining<br>Number of trials: 6/6 (6 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 11:51:04,510\tINFO tune.py:636 -- Total run time: 15.30 seconds (14.47 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.44 s, sys: 528 ms, total: 3.97 s\n",
      "Wall time: 17.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TuneGridSearchCV(early_stopping=<ray.tune.schedulers.async_hyperband.AsyncHyperBandScheduler object at 0x7fe4129ab410>,\n",
       "                 estimator=SGDClassifier(),\n",
       "                 loggers=[<class 'ray.tune.logger.JsonLogger'>,\n",
       "                          <class 'ray.tune.logger.CSVLogger'>],\n",
       "                 max_iters=10, mode='max', n_jobs=-1, name='AcademyTraining',\n",
       "                 param_grid={'alpha': [0.0001, 0.1, 1], 'epsilon': [0.01, 0.1]},\n",
       "                 scoring={'score': <function _passthrough_scorer at 0x7fe4105d2050>},\n",
       "                 sk_n_jobs=1, verbose=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tune_search = TuneGridSearchCV(\n",
    "    SGDClassifier(), parameter_grid, early_stopping=True, \n",
    "    max_iters=10, name=\"AcademyTraining\", verbose=1)\n",
    "tune_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found were:  {'alpha': 0.1, 'epsilon': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best hyperparameters found were: \", tune_search.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using Bayesian Optimization\n",
    "\n",
    "In addition to the grid search interface, tune-sklearn also provides an interface, `TuneSearchCV`, for sampling from **distributions of hyperparameters**.\n",
    "\n",
    "In addition, you can easily enable Bayesian optimization over the distributions in only 2 lines of code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-21 11:51:51 (running for 00:00:02.90)<br>Memory usage on this node: 17.8/32.0 GiB<br>Using AsyncHyperBand: num_stopped=1\n",
       "Bracket: Iter 4.000: 0.9371872580332945 | Iter 1.000: 0.8677930216802168<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/9.33 GiB heap, 0.0/4.66 GiB objects<br>Result logdir: /Users/jules/ray_results/_Trainable_2022-02-21_11-51-48<br>Number of trials: 3/3 (3 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 11:51:51,516\tINFO tune.py:636 -- Total run time: 3.01 seconds (2.89 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 327 ms, sys: 67.6 ms, total: 394 ms\n",
      "Wall time: 3.16 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TuneSearchCV(early_stopping=<ray.tune.schedulers.async_hyperband.AsyncHyperBandScheduler object at 0x7fe425428e10>,\n",
       "             estimator=SGDClassifier(),\n",
       "             loggers=[<class 'ray.tune.logger.JsonLogger'>,\n",
       "                      <class 'ray.tune.logger.CSVLogger'>],\n",
       "             max_iters=10, mode='max', n_jobs=-1, n_trials=3,\n",
       "             param_distributions={'alpha': (0.0001, 1), 'epsilon': (0.01, 0.1)},\n",
       "             scoring={'score': <function _passthrough_scorer at 0x7fe4105d2050>},\n",
       "             search_optimization='bayesian', sk_n_jobs=1, verbose=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "digits = datasets.load_digits()\n",
    "x = digits.data\n",
    "y = digits.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2)\n",
    "\n",
    "clf = SGDClassifier()\n",
    "parameter_grid = {\"alpha\": (1e-4, 1), \"epsilon\": (0.01, 0.1)}\n",
    "\n",
    "bayopt_tune_search = TuneSearchCV(\n",
    "    clf,\n",
    "    parameter_grid,\n",
    "    search_optimization=\"bayesian\",\n",
    "    n_trials=3,\n",
    "    early_stopping=True,\n",
    "    max_iters=10,\n",
    "    verbose=1,\n",
    ")\n",
    "bayopt_tune_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found were:  {'alpha': 0.31824533288476503, 'epsilon': 0.010387697631192022}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best hyperparameters found were: \", bayopt_tune_search.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise\n",
    "\n",
    " * Try increasing the `n_samples` to 110K and `test_size=10000.` \n",
    " \n",
    " Run end-to-end. If the normal scikit-learn takes too long, stop it and continue with Ray's version.\n",
    " Do you see the difference in execution time?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
