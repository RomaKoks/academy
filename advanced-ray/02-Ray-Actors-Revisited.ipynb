{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Actors Revisited\n",
    "\n",
    "Â© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../images/AnyscaleAcademyLogo.png)\n",
    "\n",
    "The [Ray Crash Course](../ray-crash-course/00-Ray-Crash-Course-Overview.ipynb) introduced the core concepts of Ray's API and how they parallelize work. Specifically, we learned how to define Ray _tasks_ and _actors_, run them, and retrieve the results. \n",
    "\n",
    "This lesson explores Ray actors in greater depth, including the following:\n",
    "\n",
    "* Detached actors\n",
    "* Specifying limits on the number of invocations and retries on failure\n",
    "* Profiling actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray, time, sys, os \n",
    "import numpy as np \n",
    "sys.path.append(\"..\")\n",
    "from util.printing import pd  # convenience methods for printing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ray Dashboard URL is printed above and also part of the output dictionary item `webui_url`\n",
    "\n",
    "(When using the Anyscale platform, use the URL provided by your instructor to access the Ray Dashboard.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detached Actors\n",
    "\n",
    "[Detached actors](https://docs.ray.io/en/latest/advanced.html#detached-actors) are designed to be long-lived actors that can be referenced by name and must be explicitly cleaned up. They are not deleted automatically when references to them go out of scope, as for regular actors. \n",
    "\n",
    "Detached actors are useful for \"services\", where different tasks and actors in the application want to lookup an actor and use it.\n",
    "\n",
    "> **Note:** This is an evolving feature. Check the [documentation](https://docs.ray.io/en/latest/advanced.html#detached-actors) for the latest details.\n",
    "\n",
    "Here is an example of a \"normal\" actor definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Counter:\n",
    "    def __init__(self):\n",
    "        self.label = 'Counter'\n",
    "        self.count = 0\n",
    "    def next(self):\n",
    "        self.count += 1\n",
    "        return self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a detached instance of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter.options(name=\"Counter1\", lifetime=\"detached\").remote()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use it \"somewhere else\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ray.get_actor(\"Counter1\")\n",
    "print(ray.get([counter.next.remote() for _ in range(100)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also the notes on detached actors and actor lifecycles in the lesson [03: Ray Internals](03-Ray-Internals.ipynb). See also the [detached actors](https://docs.ray.io/en/latest/advanced.html#detached-actors) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To kill a detached actor, use `ray.kill()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.kill(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "\n",
    "This is a new feature with a few limitations, both of which will be fixed in a forthcoming release of Ray.\n",
    "\n",
    "While `ray.kill()` kills the actor, it does not remove the name from the registration table, currently. Hence, it isn't possible to reregister a new instance with the same name. \n",
    "\n",
    "If the actor was created with a configuration value of `max_restarts` not equal to zero (discussed in the next section). the actor will be restarted up to `max_restarts` time, which will be infinitely many times if the value was set to -1.\n",
    "\n",
    "A `no_restart=True|False` keyword argument is being added to `ray.kill()` for this situation:\n",
    "\n",
    "```python\n",
    "c = ray.get_actor(\"Counter1\")\n",
    "ray.kill(c, no_restart=True)  # new optional keyword argument\n",
    "```\n",
    "\n",
    "The `no_restart=True` will be necessary for these actors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limiting Actor Invocations and Retries on Failure\n",
    "\n",
    "> **Note:** This feature may change in a future version of Ray. See the latest details in the [Ray documentation](https://docs.ray.io/en/latest/package-ref.html#ray.remote). \n",
    "\n",
    "Two options you can pass to `ray.remote` when defining an actor affect how often it can be invoked and retrying on failure:\n",
    "\n",
    "* `max_restarts`: This specifies the maximum number of times that the actor should be restarted when it dies unexpectedly. The minimum valid value is 0 (default), which indicates that the actor doesn't need to be restarted. A value of -1 indicates that an actor should be restarted indefinitely.\n",
    "* `max_task_retries`: How many times to retry an actor task if the task fails due to a system error, e.g., the actor has died. If set to -1, the system will retry the failed task until the task succeeds, or the actor has reached its max_restart limit. If set to to a value `n` greater than 0, the system will retry the failed task up to `n` times, after which time the task will throw a `RayActorError` exception when `ray.get` attempts to retrieve a result. Note that Python exceptions are not considered system errors and will not trigger retries.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "@ray.remote(max_restarts=-1, max_task_retries=-1)\n",
    "class Foo():\n",
    "    pass\n",
    "```\n",
    "\n",
    "See the [ray.remote()](https://docs.ray.io/en/latest/package-ref.html#ray.remote) documentation for all the keyword arguments supported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overriding with config()\n",
    "\n",
    "Remote task and actor objects returned by `@ray.remote` can also be dynamically modified with the same arguments supported by `ray.remote()` using `options()` as in the following example:\n",
    "\n",
    "```python\n",
    "@ray.remote(num_cpus=2, resources={\"CustomResource\": 1})\n",
    "class Foo:\n",
    "    def method(self):\n",
    "        return 1\n",
    "Bar = Foo.options(num_cpus=1, resources=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor Performance Profiling with the Ray Dashboard\n",
    "\n",
    "In lesson [01: Ray Tasks Revisited](01-Ray-Tasks-Revisited.ipynb), we learned how to profile task performance using [ray.timeline(file)](https://ray.readthedocs.io/en/latest/package-ref.html#ray.timeline)) and a Chrome web browser to view the data. \n",
    "\n",
    "Now we'll investigate how to profile performance of Ray actors using the Ray Dashboard ([documentation](https://ray.readthedocs.io/en/latest/ray-dashboard.html#ray-dashboard)). \n",
    "\n",
    "First, let's redefine the _Conway's Game of Life_ code we used in [02: Ray Actors](../ray-crash-course/02-Ray-Actors.ipynb) in the [Ray Crash Course](../ray-crash-course/00-Ray-Crash-Course-Overview.ipynb) tutorial. We've simplified a few details and pulled the definitions of `RayConwaysRules` and `State` into `RayGame` for easier distribution of everything over a cluster.\n",
    "\n",
    "This same code will be used in the exercise below. You can also find it the file [game_of_life_2.py](game_of_life_2.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class RayGame:\n",
    "    # TODO: Game memory grows unbounded; trim older states?\n",
    "    def __init__(self, grid_size, rules_ref):\n",
    "        self.states = [RayGame.State(size = grid_size)]\n",
    "        self.rules_ref = rules_ref\n",
    "\n",
    "    def get_states(self):\n",
    "        return self.states\n",
    "\n",
    "    def step(self, num_steps = 1):\n",
    "        \"\"\"Take 1 or more steps, returning a list of new states.\"\"\"\n",
    "        start_index = len(self.states)\n",
    "        for _ in range(num_steps):\n",
    "            new_state_ref = self.rules_ref.step.remote(self.states[-1])\n",
    "            self.states.append(ray.get(new_state_ref))\n",
    "        return self.states[start_index:-1]  # return the new states only!\n",
    "\n",
    "    @ray.remote\n",
    "    class RayConwaysRules:\n",
    "        \"\"\"\n",
    "        Apply the rules to a state and return a new state.\n",
    "        \"\"\"\n",
    "        def step(self, state):\n",
    "            \"\"\"\n",
    "            Determine the next values for all the cells, based on the current\n",
    "            state. Creates a new State with the changes.\n",
    "            \"\"\"\n",
    "            new_grid = state.grid.copy()\n",
    "            for i in range(state.size):\n",
    "                for j in range(state.size):\n",
    "                    lns = self.live_neighbors(i, j, state)\n",
    "                    new_grid[i][j] = self.apply_rules(i, j, lns, state)\n",
    "            new_state = RayGame.State(grid = new_grid)\n",
    "            return new_state\n",
    "\n",
    "        def apply_rules(self, i, j, live_neighbors, state):\n",
    "            \"\"\"\n",
    "            Determine next value for a cell, which could be the same.\n",
    "            The rules for Conway's Game of Life:\n",
    "                Any live cell with fewer than two live neighbours dies, as if by underpopulation.\n",
    "                Any live cell with two or three live neighbours lives on to the next generation.\n",
    "                Any live cell with more than three live neighbours dies, as if by overpopulation.\n",
    "                Any dead cell with exactly three live neighbours becomes a live cell, as if by reproduction.\n",
    "            \"\"\"\n",
    "            cell = state.grid[i][j]  # default value is no change in state\n",
    "            if cell == 1:\n",
    "                if live_neighbors < 2 or live_neighbors > 3:\n",
    "                    cell = 0\n",
    "            elif live_neighbors == 3:\n",
    "                cell = 1\n",
    "            return cell\n",
    "\n",
    "        def live_neighbors(self, i, j, state):\n",
    "            \"\"\"\n",
    "            Wrap at boundaries (i.e., treat the grid as a 2-dim \"toroid\")\n",
    "            To wrap at boundaries, when k-1=-1, that wraps itself;\n",
    "            for k+1=state.size, we mod it (which works for -1, too)\n",
    "            For simplicity, we count the cell itself, then subtact it\n",
    "            \"\"\"\n",
    "            s = state.size\n",
    "            g = state.grid\n",
    "            return sum([g[i2%s][j2%s] for i2 in [i-1,i,i+1] for j2 in [j-1,j,j+1]]) - g[i][j]\n",
    "\n",
    "    class State:\n",
    "        \"\"\"\n",
    "        Represents a grid of game cells.\n",
    "        For simplicity, require square grids.\n",
    "        Each instance is considered immutable.\n",
    "        \"\"\"\n",
    "        def __init__(self, grid = None, size = 10):\n",
    "            \"\"\"\n",
    "            Create a State. Specify either a grid of cells or a size, for\n",
    "            which an size x size grid will be computed with random values.\n",
    "            (For simplicity, only use square grids.)\n",
    "            \"\"\"\n",
    "            if type(grid) != type(None): # avoid annoying AttributeError\n",
    "                assert grid.shape[0] == grid.shape[1]\n",
    "                self.size = grid.shape[0]\n",
    "                self.grid = grid.copy()\n",
    "            else:\n",
    "                self.size = size\n",
    "                # Seed: random initialization\n",
    "                self.grid = np.random.randint(2, size = size*size).reshape((size, size))\n",
    "\n",
    "\n",
    "        def living_cells(self):\n",
    "            \"\"\"\n",
    "            Returns ([x1, x2, ...], [y1, y2, ...]) for all living cells.\n",
    "            Simplifies graphing.\n",
    "            \"\"\"\n",
    "            cells = [(i,j) for i in range(self.size) for j in range(self.size) if self.grid[i][j] == 1]\n",
    "            return zip(*cells)\n",
    "\n",
    "        def __str__(self):\n",
    "            s = ' |\\n| '.join([' '.join(map(lambda x: '*' if x else ' ', self.grid[i])) for i in range(self.size)])\n",
    "            return '| ' + s + ' |'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a timing function similar to one used in the other lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_ray_games(num_games = 1, max_steps = 100, batch_size = 1, grid_size = 100):\n",
    "    rules_refs = []\n",
    "    game_refs = []\n",
    "    for i in range(num_games):\n",
    "        rules_ref = RayGame.RayConwaysRules.remote()\n",
    "        game_ref  = RayGame.remote(grid_size, rules_ref)\n",
    "        game_refs.append(game_ref)\n",
    "        rules_refs.append(rules_ref)\n",
    "    print(f'rules_refs:\\n{rules_refs}')  # these will produce more interesting flame graphs!\n",
    "    print(f'game_refs:\\n{game_refs}')\n",
    "    start = time.time()\n",
    "    state_refs = []\n",
    "    for game_ref in game_refs:\n",
    "        for i in range(int(max_steps/batch_size)):  # Do a total of max_steps game steps, which is max_steps/delta_steps\n",
    "            state_refs.append(game_ref.step.remote(batch_size))\n",
    "    ray.get(state_refs)  # wait for everything to finish! We are ignoring what ray.get() returns, but what will it be??\n",
    "    pd(time.time() - start, prefix = f'Total time for {num_games} games (max_steps = {max_steps}, batch_size = {batch_size})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray Dashboard Profiling\n",
    "\n",
    "The Ray Dashboard provides an easy way to profile execution of Ray actors, producing [flame graphs](http://www.brendangregg.com/flamegraphs.html), which show performance characteristics of the application. This feature uses [py-spy](https://github.com/benfred/py-spy) to instrument and profile the application.\n",
    "\n",
    "> **WARNING:** Py-spy requires `sudo` access. When you follow the instructions we are about to give, you may see a message that `sudo` is required, but there's no way to enter the password in the Dashboard. \n",
    "\n",
    "You won't see this issue if you are using the Anyscale platform for this tutorial; `sudo` access is already setup as needed.\n",
    "\n",
    "If you are working on your laptop, we discuss fixes and workarounds for the `sudo` issue in [Troubleshooting, Tips, and Tricks](reference/Troubleshooting-Tips-Tricks.ipynb#Profiling-Actors). For example, one workaround is to run the application you want to profile outside a notebook, using a command line. Then if the Dashboard needs a password, you will be prompted at the terminal. \n",
    "\n",
    "In any case, we'll describe the process of profiling here and provide a demonstration video and at live tutorial events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To profile with the Dashboard, click the _Logical View_ tab. It shows a list of actors that have been executed or are running. Find the running actor that appears to be the one you want to profile. You'll see a line like this:\n",
    "\n",
    "> Actor <hex_number> (Profile for 10s 30s 60s) Kill Actor\n",
    "\n",
    "The _10s, 30s, 60s_ are links. We'll use of them to profile our `RayGame` performance.\n",
    "\n",
    "More specifically, use this procedure, which helps you find the correct actor:\n",
    "\n",
    "1. Start the next cell\n",
    "2. Copy to the clipboard the hex number shown, something like `[Actor(RayGame, a139e2970100)]`\n",
    "3. Immediately go to the Dashboard's _Logical View_\n",
    "4. Search for the actor, CTRL-F (Windows/Linux) or CMD-F (MacOS) and enter the hex code\n",
    "3. For the two actors found, one for `RayGame` and one for `RayGame.RayConwaysRules`, click _10s_ to profile them.\n",
    "4. When the profile run finishes, click _Profile results_ for each one to see the _flame graphs_ in other tabs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time time_ray_games(num_games = 1, max_steps = 400, batch_size = 50, grid_size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the top of each graph click the small left or right arrow next to _py-spy_ to see different pages of output. \n",
    "\n",
    "Cycle between _Time Order_, _Left Heavy_, and _Sandwich_ in the upper-left hand corner to see how they change the displayed information. Often _Left Heavy_ is most immediately useful.\n",
    "\n",
    "For the `RayGame` graph, there is little interesting information, as mostly it waits for `RayConwaysRules` to crunch through grids of numbers. The function calls you see are primarily networking and actor messaging background:\n",
    "![Conway's GoL Flame Graph](../images/RayGame-FlameGraph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `RayConwaysRules` display is more interesting. Here is a screen show of the _Time Order_ view _flame graph_.\n",
    "![Conway's GoL Flame Graph](../images/RayConwaysRules-FlameGraph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are looking at the call stack, with the top-most stack frame at the bottom. It's showing that the _list comprehension_ in the `ConwaysRules.live_neighbors` method, along with the rest of that method's work, are taking the most time. That's where you would want to optimize the performance, if possible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Tip:** \n",
    ">\n",
    "> 1. This view is called the _speedscope_ view of the data, which shows the flame graph. You can learn more about navigating and using this tool at the [speedscope GitHub site](https://github.com/jlfwong/speedscope).\n",
    "> 2. The [Ray Dashboard documentation](https://ray.readthedocs.io/en/latest/ray-dashboard.html#debugging-a-blocked-actor) offers tips for using the _Logical View_ to debug actor issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - \"Homework\"\n",
    "\n",
    "This exercise is more involved and will mostly appeal to those of you who like the challenges of optimizing low-level performance. Also, for a live tutorial event, this exercise is too much to take on in the limite time available. That's why it's labelled _Homework_. I encourage you to read the solution though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already proved that running whole games as actors gives us a performance boost when we need to run many of them at once, simply by running the games concurrently across the available CPU cores.\n",
    "\n",
    "This exercise explores whether or not we can improve the performance of a single game. This task isn't easy. We'll try a few ideas, but find that many don't provide much improvement. Only doing low-level optimizations provide significant improvement at this point. That could be important in a massive system where every bit of efficiency matters, but _Premature optimization is the root of all evil_. Why? Because optimizations often obscure the logic of the code, making it harder to maintain and bugs more likely. This code does basic math, but a lot of it, and sometimes it's better to crank through it in a single thread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the same code again. This time watch the _Machine View_ of the Ray Dashboard. How is the load distributed over the workers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time time_ray_games(num_games = 1, max_steps = 400, batch_size = 50, grid_size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You notice that **one** core was pegged running `RayConwaysRules.step()`. Profiling the result and looking at the flame graph provides more information. As we showed previously in this lesson, most of the time is taken up in `live_neighbors()` and in that method, more than half the time is spent in the _list comprehension_.\n",
    "\n",
    "For the exercise, there are a few things you could try.\n",
    "\n",
    "Since profiling shows that `live_neighbors` is the bottleneck, what could be done to reduce its execution time? \n",
    "\n",
    "The solution discussed in the [solutions notebook](solutions/Advanced-Ray-Solutions.ipynb) notebook shows that in fact you can reduce its overhead by about 40%. Not bad. The trick is to process the grid updates in parallel, in. blocks of rows at a time, rather than synchronously iterating through the grid cells (i.e., a \"block\" the size of the whole grid). For larger and larger game sizes, the improvement should be more noticeable.\n",
    "\n",
    "But let's step back for a moment; this is the sort of optimization you do when you _really_ have a compelling reason to squeeze optimal performance out of the code. Hence, for this exercise, it's probably overkill, unless you're interested in low-level performance optimizations like this. If you are, see the discussion in the _Solutions_ notebook.\n",
    "\n",
    "Other, easier experiments you can try may not not produce much improvement, based on the flame graph results above, but consider trying them for \"practice\". Look at `RayGame.step()` and `RayConwaysRules.step()`. There are a bunch of remote calls in there. What refactoring could be done that might improve performance? For example, what about extending `RayConwaysRules.step()` to accept a `num_steps` argument like `RayGames2.step()` supports, then modify the call to it from `RayGames.step()`? Does this actually improve performance or not? Don't forget to watch the Dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()  # \"Undo ray.init()\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next lesson, [Ray Internals](03-Ray-Internals.ipynb), explores the architecture of Ray, task scheduling, the Object Store, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "382.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
