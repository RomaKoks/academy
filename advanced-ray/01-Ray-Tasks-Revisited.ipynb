{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Tasks Revisited\n",
    "\n",
    "Â© 2019-2021, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../images/AnyscaleAcademyLogo.png)\n",
    "\n",
    "The [Ray Crash Course](../ray-crash-course/00-Ray-Crash-Course-Overview.ipynb) introduced the core concepts of Ray's API and how they parallelize work. Specifically, we learned how to define Ray _tasks_ and _actors_, run them, and retrieve the results. \n",
    "\n",
    "This lesson explores Ray tasks in greater depth, including the following:\n",
    "\n",
    "* How task dependencies are handled automatically by Ray\n",
    "* Usage patterns for `ray.get()` and `ray.wait()`\n",
    "* Specifying limits on the number of invocations and retries on failure\n",
    "* An exploration of task granularity considerations\n",
    "* Profiling tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray, time, os, sys \n",
    "import numpy as np \n",
    "sys.path.append(\"..\")\n",
    "from util.printing import pd, pnd  # convenience methods for printing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ray Dashboard URL is printed above and also part of the output dictionary item `webui_url`\n",
    "\n",
    "(When using the Anyscale platform, use the URL provided by your instructor to access the Ray Dashboard.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Task Dependencies\n",
    "\n",
    "Let's define a few remote tasks, which will have _dependency_ relationships. We'll learn how Ray handles these dependent, asynchronous computations.\n",
    "\n",
    "One task will return a random NumPy array of some size `n` and the other task will add two such arrays. We'll also add a sleep time, one tenth the size of `n` to simulate expensive computation.\n",
    "\n",
    "> **Note:** Dependencies and how Ray implements handling of them are explored in depth in the [03: Ray Internals](03-Ray-Internals.ipynb) lesson. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def make_array(n):\n",
    "    time.sleep(n/10.0)\n",
    "    return np.random.standard_normal(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a task that can add two NumPy arrays together. The arrays need to be the same size, but we'll ignore any checking for this requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def add_arrays(a1, a2):\n",
    "    time.sleep(a1.size/10.0)\n",
    "    return np.add(a1, a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets use them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "ref1 = make_array.remote(20)\n",
    "ref2 = make_array.remote(20)\n",
    "ref3 = add_arrays.remote(ref1, ref2)\n",
    "print(ray.get(ref3))\n",
    "pd(time.time() - start, prefix=\"Total time:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something subtle and \"magical\" happened here; when we called `add_arrays`, we didn't need to call `ray.get()` first for `ref1` and `ref2`, since `add_arrays` expects NumPy arrays. Because `add_arrays` is a Ray task, Ray automatically does the extraction for us, so we can write code that looks more natural and Pythonic.\n",
    "\n",
    "Furthermore, note that the `add_arrays` task effectively depends on the outputs of the two `make_array` tasks. Ray won't run `add_arrays` until the other tasks are finished. Hence, Ray automatically handles task dependencies for us.\n",
    "\n",
    "This is why the elapsed time is about 4 seconds. We used a size of 20, so we slept 2 seconds in each call to `make_array`, but those happened in parallel, _followed_ by a second sleep of 2 seconds in `add_arrays`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though three task invocations occurred, we only used one call to `ray.get()`, when we actually needed the final results. Eliminating unnecessary `ray.get()` calls helps avoid forcing tasks to become synchronous when they could be asynchronous. So, keep these two key points in mind:\n",
    "\n",
    "* _Don't ask for results you don't need._\n",
    "* _Don't ask for the results you need until you really need them._\n",
    "\n",
    "We don't need to see the objects for `id1` and `id2`. We only need the final array for `id3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ray.wait() with ray.get()\n",
    "\n",
    "Here is an idiomatic way to use `ray.get()`, where we fire all five asynchronous tasks at once, then ask for all the results at once with `ray.get()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Comprehension list: five NumPy object references or futures created\n",
    "array_refs = [make_array.remote(n*10) for n in range(5)]\n",
    "\n",
    "# Comprehension list: object references or futures of the result of addition\n",
    "added_array_refs = [add_arrays.remote(ref, ref) for ref in array_refs]\n",
    "\n",
    "# Iterate o er the list of object references or futures\n",
    "for array in ray.get(added_array_refs):\n",
    "    print(f'{array.size}: {array}')\n",
    "pd(time.time() - start, prefix=\"Total time:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes about eight seconds, four seconds for the longest invocation invocation of `make_array`, `make_array(4)`  , and four seconds with longest invocation of `add_arrays`, when passed the results of `make_array(4)`. \n",
    "\n",
    "We did the right thing inside each list comprehension. We started the asynchronous tasks all at once and allowed Ray to handle the dependencies. Then we waited on one `ray.get()` call for all the output. \n",
    "\n",
    "However, what you see is no output and then everything is suddenly printed at once after eight seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two fundamental problems with the way we've used `ray.get()` so far:\n",
    "\n",
    "1. There's no timeout, in case something gets \"hung\".\n",
    "2. We have to wait for _all_ the objects to be available before `ray.get()` returns.\n",
    "\n",
    "The ability to specify a timeout is essential in production code as a defensive measure. Many potential problems could happen in a real production system, any one of which could cause the task we're waiting on to take an abnormally long time to complete or never complete. Our application would be deadlocked waiting on this task. Hence, it's **strongly recommended** in production software to always use timeouts on blocking calls, so that the application can attempt some sort of recovery in situations like this, or at least report the error and \"degrade gracefully\".\n",
    "\n",
    "Actually, there _is_ a `timeout=<value>` option you can pass to `ray.get()` ([documentation](https://ray.readthedocs.io/en/latest/package-ref.html#ray.get)), but it will most likely be removed in a future release of Ray. Why remove it if timeouts are important? This change will simplify the implementation of `ray.get()` and encourage the use of `ray.wait()` for waiting ([documentation](https://ray.readthedocs.io/en/latest/package-ref.html#ray.wait)) instead, followed by using `ray.get()` to retrieve values for tasks that `ray.wait()` tells us are finished. \n",
    "\n",
    "Using `ray.wait()` is also the way to fix the second problem with using `ray.get()` by itself, that we have to wait for all tasks to finish before we get any values back. Some of those tasks finish more quickly in our contrived example. We would like to process those results as soon as they are available, even while others continue to run. We'll use `ray.wait()` for this purpose.\n",
    "\n",
    "Therefore, while `ray.get()` is simple and convenient, for _production code_, we recommend using `ray.wait()`, **with** timeouts, for blocking on running tasks. Then use `ray.get()` to retrieve values of completed tasks. \n",
    "\n",
    "Here is the previous example rewritten to use `ray.wait()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "array_refs = [make_array.remote(n*10) for n in range(5)]\n",
    "added_array_refs = [add_arrays.remote(ref, ref) for ref in array_refs]\n",
    "\n",
    "arrays = []\n",
    "waiting_refs = list(added_array_refs)  # Assign a working list to the full list of refs\n",
    "while len(waiting_refs) > 0:           # Loop until all tasks have completed\n",
    "    # Call ray.wait with:\n",
    "    #   1. the list of refs we're still waiting to complete,\n",
    "    #   2. tell it to return immediately as soon as one of them completes,\n",
    "    #   3. tell it wait up to 10 seconds before timing out.\n",
    "    ready_refs, remaining_refs = ray.wait(waiting_refs, num_returns=1, timeout=10.0)\n",
    "    print('Returned {:3d} completed tasks. (elapsed time: {:6.3f})'.format(len(ready_refs), time.time() - start))\n",
    "    new_arrays = ray.get(ready_refs)\n",
    "    arrays.extend(new_arrays)\n",
    "    for array in new_arrays:\n",
    "        print(f'{array.size}: {array}')\n",
    "    waiting_refs = remaining_refs  # Reset this list; don't include the completed refs in the list again!\n",
    "    \n",
    "print(f\"\\nall arrays: {arrays}\")\n",
    "pd(time.time() - start, prefix=\"Total time:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it still takes about 8 seconds to complete, 4 seconds for the longest invocation of `make_array` and 4 seconds for the invocation of `add_arrays`, but since the others complete more quickly, we see their results as soon as they become available, at 0, 2, 4, and 6 second intervals.\n",
    "\n",
    "> **Warning:** For each call to `ray.wait()` in a loop like this, it's important to remove the refs that have completed. Otherwise, `ray.wait()` will return immediately with the same list containg the first completed item, over and over again; you'll loop forever!! Resetting the list is easy, since the second list returned by `ray.wait()` is the rest of the items that are still running. So, that's what we use.\n",
    "\n",
    "Now let's try it with `num_returns = 2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "array_refs = [make_array.remote(n*10) for n in range(5)]\n",
    "added_array_refs = [add_arrays.remote(ref, ref) for ref in array_refs]\n",
    "\n",
    "arrays = []\n",
    "waiting_refs = list(added_array_refs)  # Assign a working list to the full list of refs\n",
    "while len(waiting_refs) > 0:           # Loop until all tasks have completed\n",
    "    # Call ray.wait with:\n",
    "    #   1. the list of refs we're still waiting to complete,\n",
    "    #   2. tell it to return immediately as soon as TWO of them complete,\n",
    "    #   3. tell it wait up to 10 seconds before timing out.\n",
    "    return_n = 2 if len(waiting_refs) > 1 else 1\n",
    "    ready_refs, remaining_refs = ray.wait(waiting_refs, num_returns=return_n, timeout=10.0)\n",
    "    print('Returned {:3d} completed tasks. (elapsed time: {:6.3f})'.format(len(ready_refs), time.time() - start))\n",
    "    new_arrays = ray.get(ready_refs)\n",
    "    arrays.extend(new_arrays)\n",
    "    for array in new_arrays:\n",
    "        print(f'{array.size}: {array}')\n",
    "    waiting_refs = remaining_refs  # Reset this list; don't include the completed refs in the list again!\n",
    "    \n",
    "print(f\"\\nall arrays: {arrays}\")\n",
    "pd(time.time() - start, prefix=\"Total time:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get two at a time output. Note that we don't actually pass `num_returns=2` every time. If you ask for more items than the length of the input list, you get an error. So, we compute `num_returns`, using `2` except when there's only one task to wait on, in which case we use `1`. So, in fact, the output for `40` was a single task result, because we started with `5` and processed two at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For a longer discussion on `ray.wait()`, see [this blog post](https://medium.com/distributed-computing-with-ray/ray-tips-and-tricks-part-i-ray-wait-9ed7a0b9836d)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "The following cell is identical to the last one. Modify it to use a timeout of `2.5` seconds, shorter than our longest tasks. What happens now? Try using other times.\n",
    "\n",
    "See the [solutions notebook](solutions/Advanced-Ray-Solutions.ipynb) for a discussion of this exercise and the subsequent exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "array_refs = [make_array.remote(n*10) for n in range(5)]\n",
    "added_array_refs = [add_arrays.remote(ref, ref) for ref in array_refs]\n",
    "\n",
    "arrays = []\n",
    "waiting_refs = list(added_array_refs)  # Assign a working list to the full list of refs\n",
    "while len(waiting_refs) > 0:           # Loop until all tasks have completed\n",
    "    # Call ray.wait with:\n",
    "    #   1. the list of refs we're still waiting to complete,\n",
    "    #   2. tell it to return immediately as soon as TWO of them complete,\n",
    "    #   3. tell it wait up to 10 seconds before timing out.\n",
    "    return_n = 2 if len(waiting_refs) > 1 else 1\n",
    "    ready_refs, remaining_refs = ray.wait(waiting_refs, num_returns=return_n, timeout=10.0)\n",
    "    print('Returned {:3d} completed tasks. (elapsed time: {:6.3f})'.format(len(ready_refs), time.time() - start))\n",
    "    new_arrays = ray.get(ready_refs)\n",
    "    arrays.extend(new_arrays)\n",
    "    for array in new_arrays:\n",
    "        print(f'{array.size}: {array}')\n",
    "    waiting_refs = remaining_refs  # Reset this list; don't include the completed refs in the list again!\n",
    "    \n",
    "print(f\"\\nall arrays: {arrays}\")\n",
    "pd(time.time() - start, prefix=\"Total time:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion:\n",
    "\n",
    "> **Tips:**\n",
    ">\n",
    "> 1. Use `ray.wait()` with a timeout to wait for one or more running tasks. Then use `ray.get()` to retrieve the values for the finished tasks.\n",
    "> 2. When looping over calls to `ray.wait()` with a list of object refs for running tasks, remove the previously-completed and retrieved objects from the list.\n",
    "> 3. Don't ask for results you don't need.\n",
    "> 4. Don't ask for the results you need until you really need them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Let's practice converting a slow loop to Ray, including using `ray.wait()`. Change the function to be a Ray task. Change the invocations to use the `ray.wait()` idiom. You can just use the default values for `num_returns` and `timeout` if you want. The second cell uses `assert` statements to check your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slow_square(n):\n",
    "    time.sleep(n)\n",
    "    return n*n\n",
    "\n",
    "start = time.time()\n",
    "squares = [slow_square(n) for n in range(4)]\n",
    "for square in squares:\n",
    "    print (f'finished: {square}')\n",
    "duration = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert squares == [0, 1, 4, 9], f'Did you use ray.get() to retrieve the values? squares = {squares}'\n",
    "assert duration < 4.1, f'Did you use Ray to parallelize the work? duration = {duration}' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limiting Task Invocations and Retries on Failure\n",
    "\n",
    "> **Note:** This feature may change in a future version of Ray. See the latest details in the [Ray documentation](https://docs.ray.io/en/latest/package-ref.html#ray.remote). \n",
    "\n",
    "Two options you can pass to `ray.remote` when defining a task affect how often it can be invoked and retrying on failure:\n",
    "\n",
    "* `max_calls`: This specifies the maximum number of times that a given worker can execute the given remote function before it must exit. This can be used to address memory leaks in third-party libraries or to reclaim resources that cannot easily be released, e.g., GPU memory that was acquired by TensorFlow. By default this is infinite.\n",
    "* `max_retries`: This specifies the maximum number of times that the remote function should be rerun when the worker process executing it crashes unexpectedly. The minimum valid value is 0, the default is 4, and a value of -1 indicates infinite retries are allowed.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "@ray.remote(max_calls=10000, max_retries=10)\n",
    "def foo():\n",
    "    pass\n",
    "```\n",
    "\n",
    "See the [ray.remote()](https://docs.ray.io/en/latest/package-ref.html#ray.remote) documentation for all the keyword arguments supported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overriding with config()\n",
    "\n",
    "Remote task and actor objects returned by `@ray.remote` can also be dynamically modified with the same arguments supported by `ray.remote()` using `options()` as in the following examples:\n",
    "\n",
    "```python\n",
    "@ray.remote(num_gpus=1, max_calls=1, num_return_vals=2)\n",
    "def f():\n",
    "    return 1, 2\n",
    "g = f.options(num_gpus=2, max_calls=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is the Optimal Task Granularity\n",
    "\n",
    "How fine-grained should Ray tasks be? There's no fixed rule of thumb, but Ray clearly adds some overhead for task management and using object stores in a cluster. Therefore, it makes sense that tasks which are too small will perform poorly.\n",
    "\n",
    "We'll explore this topic over several more lessons, but for now, let's get a sense of the overhead while running in your setup.\n",
    "\n",
    "We'll continue to use NumPy arrays to create \"load\", but remove the `sleep` calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noop(n):\n",
    "    return n\n",
    "\n",
    "def local_make_array(n):\n",
    "    return np.random.standard_normal(n)\n",
    "\n",
    "@ray.remote\n",
    "def remote_make_array(n):\n",
    "    return local_make_array(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do `trials` runs for each experiment, to average out background noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's use `noop` to baseline local function calls. Note that we call `print` for the duration, rathern than `pd`, because the overhead is so low the `pd` formatting will print `0.000`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "[noop(t) for t in range(trials)]\n",
    "print(f'{time.time() - start} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the same run with `local_make_array(n)` for `n = 100000`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "[local_make_array(100000) for _ in range(trials)]\n",
    "print(f'{time.time() - start} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can safely ignore the \"noop\" overhead for now. For completeness, here's what happens with remote execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "refs = [remote_make_array.remote(100000) for _ in range(trials)]\n",
    "ray.get(refs)\n",
    "print(f'{time.time() - start} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For arrays of 100000, using Ray is faster (at least on this test machine). The benefits of parallel computation, rather than synchronous, already outweight the Ray overhead.\n",
    "\n",
    "So, let's run some trials with increasingly large array sizes, to compare the performance with local vs. remote execution. First, we'll set up `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_durations = []\n",
    "remote_durations = []\n",
    "# These n values were determined by experimentation on this test machine. \n",
    "# If you are using an old machine, and this cell takes a long time to execute,\n",
    "# you could set the `trials` value above to a smaller number. \n",
    "ns = [i*(10**j) for j in range(2,5) for i in [1,2,3,5,8]]\n",
    "for n in ns:\n",
    "    start_local = time.time()\n",
    "    [local_make_array(n) for _ in range(trials)]\n",
    "    local_durations.append(time.time() - start_local)\n",
    "    \n",
    "    start_remote = time.time()\n",
    "    refs = [remote_make_array.remote(n) for _ in range(trials)]\n",
    "    ray.get(refs)\n",
    "    remote_durations.append(time.time() - start_remote)\n",
    "(ns, local_durations, remote_durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "\n",
    "import bokeh.io\n",
    "# The next two lines prevent Bokeh from opening the graph in a new window.\n",
    "bokeh.io.reset_output()\n",
    "bokeh.io.output_notebook()\n",
    "\n",
    "tooltips = [\n",
    "    (\"name\", \"$name\"),\n",
    "    (\"array size\", \"$x\"),\n",
    "    (\"time\", \"$y\")]\n",
    "p1 = figure(x_axis_type=\"log\", y_axis_type=\"log\", title=\"Execution Times\", tooltips=tooltips)\n",
    "p1.grid.grid_line_alpha=0.3\n",
    "p1.xaxis.axis_label = 'array size'\n",
    "p1.yaxis.axis_label = 'time'\n",
    "\n",
    "p1.line(ns, local_durations, color='#A6CEE3', legend_label='local', name='local')\n",
    "p1.circle(ns, local_durations, color='darkgrey', size=4)\n",
    "p1.line(ns, remote_durations, color='#B2DF8A', legend_label='remote', name='remote')\n",
    "p1.square(ns, remote_durations, color='darkgrey', size=4)\n",
    "p1.legend.location = \"top_left\"\n",
    "\n",
    "show(gridplot([[p1]], plot_width=800, plot_height=400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't see the graph? Here's [one from a prior test run](../images/Execution-Times-Local-v-Remote.png). Your results may look a lot different!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm what the graph shows as the crossing point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "while i < len(ns) and local_durations[i] < remote_durations[i]:\n",
    "    i=i+1\n",
    "print('The Ray times are faster starting at n = {:d}, local = {:6.3f} vs. remote = {:6.3f}'.format(\n",
    "    ns[i], local_durations[i], remote_durations[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling Tasks with ray.timeline()\n",
    "\n",
    "Sometimes you need to debug performance problems in Ray tasks. Calling `ray.timeline(file)` ([documentation](https://ray.readthedocs.io/en/latest/package-ref.html#ray.timeline)) captures profiling information for subsequent task execution to the specified file. Afterwards, you can view the data in a Chrome web browser. The format used is unique to Chrome, so Chrome has be used to view the data.\n",
    "\n",
    "Let's try it with our `make_array` and `add_arrays` methods in the following code. First some potential cleanup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline_file = 'task-timeline.txt' # Will be found in the same directory as this notebook.\n",
    "if os.path.isfile(timeline_file):   # Delete old one, if an old one exists already.\n",
    "    os.remove(timeline_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.timeline(timeline_file)\n",
    "start = time.time()\n",
    "array_refs = [make_array.remote(n*10) for n in range(5)]\n",
    "added_array_refs = [add_arrays.remote(ref, ref) for ref in array_refs]\n",
    "for array in ray.get(added_array_refs):\n",
    "    print(f'{array.size}: {array}')\n",
    "pd(time.time() - start, prefix=\"Total time:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to view the data:\n",
    "\n",
    "1. Open Chrome and enter chrome://tracing.\n",
    "2. Click the _Load_ button to load the `task-timeline.txt` file, which will be in this notebook's directory. \n",
    "3. To zoom in or out, click the \"asymmetric\" up-down arrow button. Then hold the mouse button in the graph and roll the mouse scroll wheel up or down. (On a laptop trackpad, press and hold, then use another finger to slide up and down.)\n",
    "4. To move around, click the crossed arrow and drag a section in view. \n",
    "5. Click on a box in the timeline to see details about it. \n",
    "\n",
    "Look for blocks corresponding to long-running tasks and look for idle periods, which reflect processing outside the context of Ray.\n",
    "\n",
    "Here is a screen grab profiling the previous code, zoomed in on one block of tasks and with one task selected. Note the processes shown on the left for drivers (more than one notebook was running at this time) and workers.\n",
    "\n",
    "![Ray Trace Example](../images/Ray-Trace-Example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()  # \"Undo ray.init()\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next lesson, [Ray Actors Revisited](02-Ray-Actors-Revisited.ipynb), revisits actors. It provides a more in-depth look at actor characteristics and profiling actor performance using the _Ray Dashboard_."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "382.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
