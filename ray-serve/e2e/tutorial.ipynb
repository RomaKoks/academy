{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aging-patch",
   "metadata": {},
   "source": [
    "\n",
    "<a id='end-to-end-tutorial'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-arthur",
   "metadata": {},
   "source": [
    "[Open in colab](https://colab.research.google.com/github/anyscale/academy/blob/main/ray-serve/e2e/tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-possibility",
   "metadata": {},
   "source": [
    "# Model Deployment with Ray Serve: Introduction\n",
    "\n",
    "This tutorial has few goals in mind. \n",
    "\n",
    "1. Introduce you to the the landscape of ML serving tools, and where does Ray Serve fit.\n",
    "2. Teach you how to deploy any Python based model with Ray Serve, and compose them for production ready pipelines.\n",
    "3. Show you the concrete steps required to deploy models for interactive REST endpoint.\n",
    "\n",
    "If you any question about this tutorial, or any follow up questions, please feel free to ask in the [Ray discussion forum](https://discuss.ray.io/c/ray-serve/6)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-transcript",
   "metadata": {},
   "source": [
    "# 1 Landscape of ML Tools\n",
    "\n",
    "Where does Ray Serve fits in the lanscape of machine learning deployment tools? \n",
    "\n",
    "Commonly there's a spectrum of tools:\n",
    "- People typically starts with either framework specific servers (TFServing, TorchServer) or web servers (Flask, FastAPI) as an easy start to deploy a single model. \n",
    "- For more \"production-readiness\", various custom toolings are added (Docker, K8s, Golang based microservices). \n",
    "- But you can't just maintain a glued-together system. Folks starting looking for special purpose deployment tools (KubeFlow, KServe, Triton, etc) to manage and deploy many models in production. \n",
    "\n",
    "Over the spectrum, our team observe that you have to trade-off ease of development with production scalability. Ray Serve lets you easily develop locally and then transparently scale to production.\n",
    "\n",
    "![Serve aims at both ease of development and ready for production.](serve-position.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-management",
   "metadata": {},
   "source": [
    "# 2 Model Serving with Ray Serve\n",
    "\n",
    "Adapted from our [documentation](https://docs.ray.io/en/master/serve/index.html#rayserve)\n",
    "\n",
    "By the end of this tutorial you will have learned how to deploy a machine\n",
    "learning model locally via Ray Serve.\n",
    "\n",
    "First, install Ray Serve and all of its dependencies by running the following\n",
    "command in your terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "different-dinner",
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qq \"ray[serve]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-literacy",
   "metadata": {},
   "source": [
    "For this tutorial, we’ll use [HuggingFace’s SummarizationPipeline](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.SummarizationPipeline)\n",
    "to access a model that summarizes text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "classified-gambling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qq transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-theater",
   "metadata": {},
   "source": [
    "## Example Model\n",
    "\n",
    "Let’s first take a look at how the model works, without using Ray Serve.\n",
    "This is the code for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "removed-blast",
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two astronauts steered their lunar module safely and smoothly to the historic landing . the first men to reach the moon brought their ship to rest on a level, rock-strewn plain near the arid sea of Tranquility . a television camera outside the craft transmitted his every move to an awed audience .\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "def summarize(text):\n",
    "    # Load model\n",
    "    summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
    "\n",
    "    # Run inference\n",
    "    summary_list = summarizer(text)\n",
    "\n",
    "    # Post-process output to return only the summary text\n",
    "    summary = summary_list[0][\"summary_text\"]\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "article_text = (\n",
    "    \"HOUSTON -- Men have landed and walked on the moon. \"\n",
    "    \"Two Americans, astronauts of Apollo 11, steered their fragile \"\n",
    "    \"four-legged lunar module safely and smoothly to the historic landing \"\n",
    "    \"yesterday at 4:17:40 P.M., Eastern daylight time. Neil A. Armstrong, the \"\n",
    "    \"38-year-old commander, radioed to earth and the mission control room \"\n",
    "    \"here: \\\"Houston, Tranquility Base here. The Eagle has landed.\\\" The \"\n",
    "    \"first men to reach the moon -- Armstrong and his co-pilot, Col. Edwin E. \"\n",
    "    \"Aldrin Jr. of the Air Force -- brought their ship to rest on a level, \"\n",
    "    \"rock-strewn plain near the southwestern shore of the arid Sea of \"\n",
    "    \"Tranquility. About six and a half hours later, Armstrong opened the \"\n",
    "    \"landing craft\\'s hatch, stepped slowly down the ladder and declared as \"\n",
    "    \"he planted the first human footprint on the lunar crust: \\\"That\\'s one \"\n",
    "    \"small step for man, one giant leap for mankind.\\\" His first step on the \"\n",
    "    \"moon came at 10:56:20 P.M., as a television camera outside the craft \"\n",
    "    \"transmitted his every move to an awed and excited audience of hundreds \"\n",
    "    \"of millions of people on earth.\")\n",
    "\n",
    "summary = summarize(article_text)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-element",
   "metadata": {},
   "source": [
    "The Python file, called `local_model.py`, uses the `summarize` function to\n",
    "generate summaries of text.\n",
    "\n",
    "- The `summarizer` variable on line 7 inside `summarize` points to a\n",
    "  function that uses the [t5-small](https://huggingface.co/t5-small)\n",
    "  model to summarize text.  \n",
    "- When `summarizer` is called on a Python String, it returns summarized text\n",
    "  inside a dictionary formatted as `[{\"summary_text\": \"...\", ...}, ...]`.  \n",
    "- `summarize` then extracts the summarized text on line 13 by indexing into\n",
    "  the dictionary.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-disposition",
   "metadata": {},
   "source": [
    "Keep in mind that the `SummarizationPipeline` is an example machine learning\n",
    "model for this tutorial. You can follow along using arbitrary models in any\n",
    "framework that has a Python API. Check out our tutorials on sckit-learn,\n",
    "PyTorch, and Tensorflow for more info and examples:\n",
    "\n",
    "- [Keras and Tensorflow Tutorial](https://docs.ray.io/en/latest/serve/tutorials/tensorflow.html)\n",
    "- [PyTorch Tutorial](https://docs.ray.io/en/latest/serve/tutorials/pytorch.html)\n",
    "- [Scikit-Learn Tutorial](https://docs.ray.io/en/latest/serve/tutorials/sklearn.html)\n",
    "- [Batching Tutorial](https://docs.ray.io/en/latest/serve/tutorials/batch.html)\n",
    "- [RLlib Tutorial](https://docs.ray.io/en/latest/serve/tutorials/rllib.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-awareness",
   "metadata": {},
   "source": [
    "## Converting to Ray Serve Deployment\n",
    "\n",
    "This tutorial’s goal is to deploy this model using Ray Serve, so it can be\n",
    "scaled up and queried over HTTP. We’ll start by converting the above Python\n",
    "function into a Ray Serve deployment that can be launched locally on a laptop.\n",
    "\n",
    "First, we need to import `ray` and `ray serve`, to use features in Ray Serve such as `deployments`, which provide HTTP access to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "worth-criminal",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-greene",
   "metadata": {},
   "source": [
    "After these imports, we can include our model code from above.\n",
    "We won’t call our `summarize` function just yet though!\n",
    "We will soon add logic to handle HTTP requests, so the `summarize` function\n",
    "can operate on article text sent via HTTP request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "plastic-probe",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def summarize(text):\n",
    "    summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
    "    summary_list = summarizer(text)\n",
    "    summary = summary_list[0][\"summary_text\"]\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-robin",
   "metadata": {},
   "source": [
    "Ray Serve needs to run on top of a Ray cluster, so we create a local one.\n",
    "See [Deploying Ray Serve](https://docs.ray.io/en/latest/serve/deployment.html) to learn more about starting a Ray Serve\n",
    "instance and deploying to a Ray cluster.\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">You can use Ray to perform data processing, hyperparameter-tuning, and distributed model training as well! Learn more at http://ray.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "short-marking",
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-28 14:39:28,285\tINFO services.py:1412 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '127.0.0.1',\n",
       " 'raylet_ip_address': '127.0.0.1',\n",
       " 'redis_address': '127.0.0.1:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2022-01-28_14-39-24_412847_89101/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-01-28_14-39-24_412847_89101/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2022-01-28_14-39-24_412847_89101',\n",
       " 'metrics_export_port': 64097,\n",
       " 'gcs_address': '127.0.0.1:63389',\n",
       " 'address': '127.0.0.1:6379',\n",
       " 'node_id': '1a270d4109b030c049a391f40914cd03d9e33a0ff2b2abadc052ac5e'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "appointed-farming",
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=41296)\u001b[0m 2022-01-28 14:40:03,882\tINFO checkpoint_path.py:16 -- Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(ServeController pid=41296)\u001b[0m 2022-01-28 14:40:03,989\tINFO http_state.py:101 -- Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:dPuHru:SERVE_PROXY_ACTOR-node:127.0.0.1-0' on node 'node:127.0.0.1-0' listening on '127.0.0.1:8000'\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=41297)\u001b[0m INFO:     Started server process [41297]\n",
      "2022-01-28 14:40:04,456\tINFO api.py:518 -- Started Serve instance in namespace 'ca9d03ec-9ba7-4937-b1a8-9d2e3e95ad99'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.serve.api.Client at 0x7fdc21e5a7f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-motel",
   "metadata": {},
   "source": [
    "Now that we have defined our `summarize` function, connected to a Ray\n",
    "Cluster, and started the Ray Serve runtime, we can define a function that\n",
    "accepts HTTP requests and routes them to the `summarize` function. We\n",
    "define a function called `router` that takes in a Starlette `request`\n",
    "objec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "furnished-option",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "def router(request):\n",
    "    txt = request.query_params[\"txt\"]\n",
    "    return summarize(txt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-kitty",
   "metadata": {},
   "source": [
    "- In line 1, we add the decorator `@serve.deployment`\n",
    "  to the `router` function to turn the function into a Serve `Deployment`\n",
    "  object.  \n",
    "- In line 3, `router` uses the `\"txt\"` query parameter in the `request`\n",
    "  to get the article text to summarize.  \n",
    "- In line 4, it then passes this article text into the `summarize` function\n",
    "  and returns the value.  \n",
    "\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">Lines 3 and 4 define our HTTP request schema. The HTTP requests sent to this\n",
    "endpoint must have a `\"txt\"` query parameter that contains a string.\n",
    "In general, you can accept HTTP data using query parameters or the\n",
    "request body. Additionally, you can add other Serve deployments with\n",
    "different names to create more endpoints that can accept different schemas.\n",
    "For more complex validation, you can also use FastAPI (see\n",
    "[FastAPI HTTP Deployments](https://docs.ray.io/en/latest/serve/http-servehandle.html#fastapi-http-deployments) for more info).\n",
    "\n",
    "This routing function’s name doesn’t have to be `router`. Serve uses your function name to namespace the route. For example the router will be accessible via `http://localhost:8000/router?txt=your-data`. You can change the function name or explicitly provider a `route_prefix` to Ray Serve via `@serve.deployment(route_prefix=\"/...\")`. \n",
    "\n",
    "Since `@serve.deployment` makes `router` a `Deployment` object, it can be\n",
    "deployed using `router.deploy()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "duplicate-tolerance",
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-28 14:44:59,844\tINFO api.py:259 -- Updating deployment 'router'. component=serve deployment=router\n",
      "\u001b[2m\u001b[36m(ServeController pid=41296)\u001b[0m 2022-01-28 14:44:59,866\tINFO deployment_state.py:919 -- Adding 1 replicas to deployment 'router'. component=serve deployment=router\n",
      "2022-01-28 14:45:05,895\tINFO api.py:272 -- Deployment 'router' is ready at `http://127.0.0.1:8000/router`. component=serve deployment=router\n"
     ]
    }
   ],
   "source": [
    "router.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-potato",
   "metadata": {},
   "source": [
    "Once we deploy `router`, we can query the model over HTTP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-leader",
   "metadata": {},
   "source": [
    "## Testing the Ray Serve Deployment\n",
    "\n",
    "We can now test our model over HTTP. The structure of our HTTP query is:\n",
    "\n",
    "`http://127.0.0.1:8000/[Deployment Name]?[Parameter Name-1]=[Parameter Value-1]&[Parameter Name-2]=[Parameter Value-2]&...&[Parameter Name-n]=[Parameter Value-n]`\n",
    "\n",
    "Since the cluster is deployed locally in this tutorial, the `127.0.0.1:8000`\n",
    "refers to a localhost with port 8000. The `[Deployment Name]` refers to\n",
    "either the name of the function that we called `.deploy()` on (in our case,\n",
    "this is `router`), or the `name` keyword parameter’s value in\n",
    "`@serve.deployment` (see the Tip under the `router` function definition\n",
    "above for more info).\n",
    "\n",
    "Each `[Parameter Name]` refers to a field’s name in the\n",
    "request’s `query_params` dictionary for our deployed function. In our\n",
    "example, the only parameter we need to pass in is `txt`. This parameter is\n",
    "referenced in the `txt = request.query_params[\"txt\"]` line in the `router`\n",
    "function. Each [Parameter Name] object has a corresponding [Parameter Value]\n",
    "object. The `txt`’s [Parameter Value] is a string containing the article\n",
    "text to summarize. We can chain together any number of the name-value pairs\n",
    "using the `&` symbol in the request URL.\n",
    "\n",
    "Now that the `summarize` function is deployed on Ray Serve, we can make HTTP\n",
    "requests to it. Here’s a client script that requests a summary from the same\n",
    "article as the original Python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "latest-sociology",
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(router pid=41299)\u001b[0m Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "\u001b[2m\u001b[36m(router pid=41299)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[2m\u001b[36m(router pid=41299)\u001b[0m /Users/simonmo/miniconda3/lib/python3.6/site-packages/transformers/generation_utils.py:730: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "\u001b[2m\u001b[36m(router pid=41299)\u001b[0m   beam_id = beam_token_id // vocab_size\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'two astronauts steered their lunar module safely and smoothly to the historic landing . the first men to reach the moon brought their ship to rest on a level, rock-strewn plain near the arid sea of Tranquility . a television camera outside the craft transmitted his every move to an awed audience .'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "\n",
    "article_text = (\n",
    "    \"HOUSTON -- Men have landed and walked on the moon. \"\n",
    "    \"Two Americans, astronauts of Apollo 11, steered their fragile \"\n",
    "    \"four-legged lunar module safely and smoothly to the historic landing \"\n",
    "    \"yesterday at 4:17:40 P.M., Eastern daylight time. Neil A. Armstrong, the \"\n",
    "    \"38-year-old commander, radioed to earth and the mission control room \"\n",
    "    \"here: \\\"Houston, Tranquility Base here. The Eagle has landed.\\\" The \"\n",
    "    \"first men to reach the moon -- Armstrong and his co-pilot, Col. Edwin E. \"\n",
    "    \"Aldrin Jr. of the Air Force -- brought their ship to rest on a level, \"\n",
    "    \"rock-strewn plain near the southwestern shore of the arid Sea of \"\n",
    "    \"Tranquility. About six and a half hours later, Armstrong opened the \"\n",
    "    \"landing craft\\'s hatch, stepped slowly down the ladder and declared as \"\n",
    "    \"he planted the first human footprint on the lunar crust: \\\"That\\'s one \"\n",
    "    \"small step for man, one giant leap for mankind.\\\" His first step on the \"\n",
    "    \"moon came at 10:56:20 P.M., as a television camera outside the craft \"\n",
    "    \"transmitted his every move to an awed and excited audience of hundreds \"\n",
    "    \"of millions of people on earth.\")\n",
    "\n",
    "response = requests.get(\"http://127.0.0.1:8000/router?txt=\" +\n",
    "                        article_text).text\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-gates",
   "metadata": {},
   "source": [
    "## Using Classes in the Ray Serve Deployment\n",
    "\n",
    "Our application is still a bit inefficient though. In particular, the\n",
    "`summarize` function loads the model on each call when it sets the\n",
    "`summarizer` variable. However, the model never changes, so it would be more\n",
    "efficient to define `summarizer` only once and keep its value in memory\n",
    "instead of reloading it for each HTTP query.\n",
    "\n",
    "We can achieve this by converting our `summarize` function into a class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "immediate-attempt",
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-28 14:53:05,646\tINFO api.py:259 -- Updating deployment 'Summarizer'. component=serve deployment=Summarizer\n",
      "\u001b[2m\u001b[36m(ServeController pid=41296)\u001b[0m 2022-01-28 14:53:05,703\tINFO deployment_state.py:919 -- Adding 1 replicas to deployment 'Summarizer'. component=serve deployment=Summarizer\n",
      "\u001b[2m\u001b[36m(Summarizer pid=41292)\u001b[0m Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "\u001b[2m\u001b[36m(Summarizer pid=41292)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2022-01-28 14:53:13,052\tINFO api.py:272 -- Deployment 'Summarizer' is ready at `http://127.0.0.1:8000/Summarizer`. component=serve deployment=Summarizer\n"
     ]
    }
   ],
   "source": [
    "@serve.deployment\n",
    "class Summarizer:\n",
    "    def __init__(self):\n",
    "        self.summarize = pipeline(\"summarization\", model=\"t5-small\")\n",
    "\n",
    "    def __call__(self, request):\n",
    "        txt = request.query_params[\"txt\"]\n",
    "        summary_list = self.summarize(txt)\n",
    "        summary = summary_list[0][\"summary_text\"]\n",
    "        return summary\n",
    "\n",
    "\n",
    "Summarizer.deploy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-boring",
   "metadata": {},
   "source": [
    "In this configuration, we can query the `Summarizer` class directly.\n",
    "The `Summarizer` is initialized once (after calling `Summarizer.deploy()`).\n",
    "In line 13, its `__init__` function loads and stores the model in\n",
    "`self.summarize`. HTTP queries for the `Summarizer` class are routed to its\n",
    "`__call__` method by default, which takes in the Starlette `request`\n",
    "object. The `Summarizer` class can then take the request’s `txt` data and\n",
    "call the `self.summarize` function on it without loading the model on each\n",
    "query.\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">Instance variables can also store state. For example, to\n",
    ">count the number of requests served, a `@serve.deployment` class can define\n",
    ">a `self.counter` instance variable in its `__init__` function and set it\n",
    ">to 0. When the class is queried, it can increment the `self.counter`\n",
    ">variable inside of the function responding to the query. The `self.counter`\n",
    ">will keep track of the number of requests served across requests.\n",
    "\n",
    "HTTP queries for the Ray Serve class deployments follow a similar format to Ray\n",
    "Serve function deployments. Here’s an example client script for the\n",
    "`Summarizer` class. Notice that the only difference from the `router`’s\n",
    "client script is that the URL uses the `Summarizer` path instead of\n",
    "`router`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "serious-invite",
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Summarizer pid=41292)\u001b[0m /Users/simonmo/miniconda3/lib/python3.6/site-packages/transformers/generation_utils.py:730: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "\u001b[2m\u001b[36m(Summarizer pid=41292)\u001b[0m   beam_id = beam_token_id // vocab_size\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'two astronauts steered their lunar module safely and smoothly to the historic landing . the first men to reach the moon brought their ship to rest on a level, rock-strewn plain near the arid sea of Tranquility . a television camera outside the craft transmitted his every move to an awed audience .'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(\"http://127.0.0.1:8000/Summarizer?txt=\" +\n",
    "                        article_text).text\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-breath",
   "metadata": {},
   "source": [
    "## Adding Functionality with FastAPI\n",
    "\n",
    "Now suppose we want to expose additional functionality in our model. In\n",
    "particular, the `summarize` function also has `min_length` and\n",
    "`max_length` parameters. Although we could expose these options as additional\n",
    "parameters in URL, Ray Serve also allows us to add more route options to the\n",
    "URL itself and handle each route separately.\n",
    "\n",
    "Because this logic can get complex, Serve integrates with\n",
    "[FastAPI](https://fastapi.tiangolo.com/). This allows us to define a Serve\n",
    "deployment by adding the `@serve.ingress` decorator to a FastAPI app. For\n",
    "more info about FastAPI with Serve, please see [FastAPI HTTP Deployments](https://docs.ray.io/en/latest/serve/http-servehandle.html#fastapi-http-deployments).\n",
    "\n",
    "As an example of FastAPI, here’s a modified version of our `Summarizer` class\n",
    "with route options to request a minimum or maximum length of ten words in the\n",
    "summaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "incorrect-intention",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "maritime-amount",
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-28 16:19:43,564\tINFO api.py:259 -- Updating deployment 'Summarizer'. component=serve deployment=Summarizer\n",
      "\u001b[2m\u001b[36m(ServeController pid=41296)\u001b[0m 2022-01-28 16:19:43,618\tINFO deployment_state.py:881 -- Stopping 1 replicas of deployment 'Summarizer' with outdated versions. component=serve deployment=Summarizer\n",
      "\u001b[2m\u001b[36m(ServeController pid=41296)\u001b[0m 2022-01-28 16:19:45,789\tINFO deployment_state.py:919 -- Adding 1 replicas to deployment 'Summarizer'. component=serve deployment=Summarizer\n",
      "\u001b[2m\u001b[36m(Summarizer pid=41285)\u001b[0m Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "\u001b[2m\u001b[36m(Summarizer pid=41285)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2022-01-28 16:19:52,480\tINFO api.py:272 -- Deployment 'Summarizer' is ready at `http://127.0.0.1:8000/Summarizer`. component=serve deployment=Summarizer\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray import serve\n",
    "from fastapi import FastAPI\n",
    "from transformers import pipeline\n",
    "from pydantic import BaseModel, PositiveInt, constr\n",
    "from typing import Optional\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "class Request(BaseModel):\n",
    "    text: constr(min_length=1, strip_whitespace=True)\n",
    "    min_length: Optional[PositiveInt] \n",
    "    max_length: Optional[PositiveInt]\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "@serve.ingress(app)\n",
    "class Summarizer:\n",
    "    def __init__(self):\n",
    "        self.summarize = pipeline(\"summarization\", model=\"t5-small\")\n",
    "\n",
    "    @app.post(\"/\")\n",
    "    def get_summary(self, payload: Request):\n",
    "        summary_list = self.summarize(\n",
    "            payload.text, \n",
    "            min_length=payload.min_length or 0, \n",
    "            max_length=payload.max_length or 256,\n",
    "        )\n",
    "        summary = summary_list[0][\"summary_text\"]\n",
    "        return summary\n",
    "\n",
    "\n",
    "Summarizer.deploy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-testing",
   "metadata": {},
   "source": [
    "The class now exposes the same route but with the following feature:\n",
    "\n",
    "- Takes a JSON formatted POST method. \n",
    "- Validate the input text is not empty.\n",
    "- Accept optional min_length and max_length values and validate them.\n",
    "\n",
    "Notice that `Summarizer`’s methods no longer take in a Starlette `request`\n",
    "object. Instead, they take in the URL’s txt parameter directly with FastAPI’s\n",
    "[body parameter](https://fastapi.tiangolo.com/tutorial/body/)\n",
    "feature.\n",
    "\n",
    "Since we still deploy our model locally, the full URL still uses the\n",
    "localhost IP. This means each of our three routes comes after the\n",
    "`http://127.0.0.1:8000` IP and port address. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efficient-stationery",
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Summarizer pid=41285)\u001b[0m /Users/simonmo/miniconda3/lib/python3.6/site-packages/transformers/generation_utils.py:730: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "\u001b[2m\u001b[36m(Summarizer pid=41285)\u001b[0m   beam_id = beam_token_id // vocab_size\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"two astronauts steered their lunar module\"'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.post(\"http://127.0.0.1:8000/Summarizer\",\n",
    "                        json={\"text\": article_text, \"max_length\": 10}).text\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-place",
   "metadata": {},
   "source": [
    "Congratulations! You just built and deployed a machine learning model on Ray\n",
    "Serve! You should now have enough context to dive into the [Core API: Deployments](https://docs.ray.io/en/latest/serve/core-apis.html) to\n",
    "get a deeper understanding of Ray Serve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-brain",
   "metadata": {},
   "source": [
    "# 3 Deploying to Cloud Endpoint\n",
    "\n",
    "Now the Ray Serve endpoint is ready for production! It has request validation, it efficiently use memory, and you can transparently scale it out. But you can't just keep this running as a notebook for live serving instances. \n",
    "\n",
    "Here are some practical recommednations to deployment:\n",
    "- For quick demo, use [ngrok](https://ngrok.com/) to expose your local port to the internet. \n",
    "- For running on GCP for quick proof of concept, use Google Cloud Run to run a containerized application. \n",
    "- For production deployment, use [Kubernetes with Ray operator](https://docs.ray.io/en/latest/serve/deployment.html#deploying-on-kubernetes) or hosted provider like [Anyscale](anysclae.com).\n",
    "\n",
    "### Cloud Run Example\n",
    "You can take a look at the [example directory](https://github.com/anyscale/academy/tree/main/ray-serve/e2e/deploy-cloud-run) for a complete example containerizing the application and deploy to Google Coud Run. It contains\n",
    "- `deploy.py` for the application.\n",
    "- `test-query.py` for calling the application as a client.\n",
    "- `requirements.txt` for Python dependencies.\n",
    "- `Dockerfile` for describing how to containerize the application and make it runnable as docker containers.\n",
    "\n",
    "### Anyscale Example\n",
    "For deploying to Anyscale hosted service, all you need is to change set the environment variable `RAY_ADDRESS=\"anyscale://your-cluster-name` or use [production services construct](https://docs.anyscale.com/user-guide/run-and-monitor/production-services). You do need an invite at the moment though. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-going",
   "metadata": {},
   "source": [
    "# 4 Bonus: Real World Model Composition\n",
    "\n",
    ".. TODO\n",
    "\n",
    "\n",
    "!["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fancy-perfume",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-reform",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "date": 1643246257.62908,
  "filename": "end_to_end_tutorial.rst",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "title": "End-to-End Tutorial"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
