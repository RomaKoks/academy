{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Serve - Model Serving\n",
    "\n",
    "© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../images/AnyscaleAcademyLogo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll explore a nontrivial example for Ray Serve.\n",
    "\n",
    "We'll work through an example that also covers training a model, deploying it, then updating later, based on this [documentation example](https://docs.ray.io/en/latest/serve/deployment.html). This page also has a section on [deployment to Kubernetes](https://docs.ray.io/en/latest/serve/deployment.html#deploying-as-a-kubernetes-service).\n",
    "\n",
    "This example is from the Ray Serve [scikit-learn example.](https://docs.ray.io/en/latest/serve/tutorials/sklearn.html)\n",
    "See also the Serve documentation's [mini-tutorials](https://docs.ray.io/en/latest/serve/tutorials/index.html) for using Serve with various frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import serve\n",
    "import os\n",
    "import requests  # for making web requests\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 16:34:31,024\tINFO services.py:1412 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8270\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(ServeController pid=63313)\u001b[0m 2022-03-16 16:34:33,999\tINFO checkpoint_path.py:16 -- Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(ServeController pid=63313)\u001b[0m 2022-03-16 16:34:34,105\tINFO http_state.py:98 -- Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:RamarF:SERVE_PROXY_ACTOR-node:127.0.0.1-0' on node 'node:127.0.0.1-0' listening on '127.0.0.1:8000'\n",
      "2022-03-16 16:34:34,477\tINFO api.py:521 -- Started Serve instance in namespace 'serve'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.serve.api.Client at 0x7f86f8bf1d90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a Model to Serve \n",
    "\n",
    "We'll begin by training a classifier with the Iris data we used before, this time using [scikit-learn](https://scikit-learn.org/stable/). The details aren't too important for our purposes, except for the fact we'll save the trained model to disk for subsequent serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=63308)\u001b[0m INFO:     Started server process [63308]\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "iris_dataset = load_iris()\n",
    "data, target, target_names = iris_dataset[\"data\"], iris_dataset[\n",
    "    \"target\"], iris_dataset[\"target_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "model = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation split\n",
    "data, target = sklearn.utils.shuffle(data, target)\n",
    "train_x, train_y = data[:100], target[:100]\n",
    "val_x, val_y = data[100:], target[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.04\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate models\n",
    "model.fit(train_x, train_y)\n",
    "print(\"MSE:\", mean_squared_error(model.predict(val_x), val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and label to file\n",
    "MODEL_PATH = os.path.join(tempfile.gettempdir(),\n",
    "                          \"iris_model_logistic_regression.pkl\")\n",
    "LABEL_PATH = os.path.join(tempfile.gettempdir(), \"iris_labels.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and label to file. (This could also be S3 or other \"global\" place)\n",
    "\n",
    "with open(MODEL_PATH, \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "with open(LABEL_PATH, \"w\") as f:\n",
    "    json.dump(target_names.tolist(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Model and Serve It\n",
    "\n",
    "Next, we define a servable model by instantiating a class and defining the `__call__` method that Ray Serve will use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(route_prefix=\"/regressor\")\n",
    "class BoostingModel:\n",
    "    def __init__(self):\n",
    "        with open(MODEL_PATH, \"rb\") as f:\n",
    "            self.model = pickle.load(f)\n",
    "        with open(LABEL_PATH) as f:\n",
    "            self.label_list = json.load(f)\n",
    "\n",
    "    # async allows us to have this call concurrently            \n",
    "    async def __call__(self, starlette_request):\n",
    "        payload = await starlette_request.json()\n",
    "        print(\"Worker: received starlette request with data\", payload)\n",
    "\n",
    "        input_vector = [\n",
    "            payload[\"sepal length\"],\n",
    "            payload[\"sepal width\"],\n",
    "            payload[\"petal length\"],\n",
    "            payload[\"petal width\"],\n",
    "        ]\n",
    "        prediction = self.model.predict([input_vector])[0]\n",
    "        human_name = self.label_list[prediction]\n",
    "        return {\"result\": human_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 16:34:46,505\tINFO api.py:262 -- Updating deployment 'BoostingModel'. component=serve deployment=BoostingModel\n",
      "\u001b[2m\u001b[36m(ServeController pid=63313)\u001b[0m 2022-03-16 16:34:46,532\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'BoostingModel'. component=serve deployment=BoostingModel\n",
      "2022-03-16 16:34:47,284\tINFO api.py:274 -- Deployment 'BoostingModel' is ready at `http://127.0.0.1:8000/regressor`. component=serve deployment=BoostingModel\n"
     ]
    }
   ],
   "source": [
    "BoostingModel.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score the model\n",
    "Internally, Serve stores the model as a Ray actor and routes traffic to it as the endpoint is queried, in this case over HTTP. \n",
    "\n",
    "Now let’s query the endpoint to see results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_request_input = {\n",
    "    \"sepal length\": 1.2,\n",
    "    \"sepal width\": 1.0,\n",
    "    \"petal length\": 1.1,\n",
    "    \"petal width\": 0.9,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now send HTTP requests to our route `route_prefix=/regressor` at the default port 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"result\": \"versicolor\"\n",
      "}\n",
      "\u001b[2m\u001b[36m(BoostingModel pid=63311)\u001b[0m Worker: received starlette request with data {'sepal length': 1.2, 'sepal width': 1.0, 'petal length': 1.1, 'petal width': 0.9}\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(\n",
    "    \"http://localhost:8000/regressor\", json=sample_request_input)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': 'versicolor'}\n",
      "{'result': 'versicolor'}\n",
      "{'result': 'versicolor'}\n",
      "{'result': 'versicolor'}\n",
      "{'result': 'versicolor'}\n",
      "{'result': 'versicolor'}\n",
      "{'result': 'versicolor'}\n",
      "{'result': 'versicolor'}\n",
      "{'result': 'versicolor'}\n",
      "{'result': 'versicolor'}\n",
      "\u001b[2m\u001b[36m(BoostingModel pid=63311)\u001b[0m Worker: received starlette request with data {'sepal length': 1.2, 'sepal width': 1.0, 'petal length': 1.1, 'petal width': 0.9}\n",
      "\u001b[2m\u001b[36m(BoostingModel pid=63311)\u001b[0m Worker: received starlette request with data {'sepal length': 1.2, 'sepal width': 1.0, 'petal length': 1.1, 'petal width': 0.9}\n",
      "\u001b[2m\u001b[36m(BoostingModel pid=63311)\u001b[0m Worker: received starlette request with data {'sepal length': 1.2, 'sepal width': 1.0, 'petal length': 1.1, 'petal width': 0.9}\n",
      "\u001b[2m\u001b[36m(BoostingModel pid=63311)\u001b[0m Worker: received starlette request with data {'sepal length': 1.2, 'sepal width': 1.0, 'petal length': 1.1, 'petal width': 0.9}\n",
      "\u001b[2m\u001b[36m(BoostingModel pid=63311)\u001b[0m Worker: received starlette request with data {'sepal length': 1.2, 'sepal width': 1.0, 'petal length': 1.1, 'petal width': 0.9}\n",
      "\u001b[2m\u001b[36m(BoostingModel pid=63311)\u001b[0m Worker: received starlette request with data {'sepal length': 1.2, 'sepal width': 1.0, 'petal length': 1.1, 'petal width': 0.9}\n",
      "\u001b[2m\u001b[36m(BoostingModel pid=63311)\u001b[0m Worker: received starlette request with data {'sepal length': 1.2, 'sepal width': 1.0, 'petal length': 1.1, 'petal width': 0.9}\n",
      "\u001b[2m\u001b[36m(BoostingModel pid=63311)\u001b[0m Worker: received starlette request with data {'sepal length': 1.2, 'sepal width': 1.0, 'petal length': 1.1, 'petal width': 0.9}\n",
      "\u001b[2m\u001b[36m(BoostingModel pid=63311)\u001b[0m Worker: received starlette request with data {'sepal length': 1.2, 'sepal width': 1.0, 'petal length': 1.1, 'petal width': 0.9}\n",
      "\u001b[2m\u001b[36m(BoostingModel pid=63311)\u001b[0m Worker: received starlette request with data {'sepal length': 1.2, 'sepal width': 1.0, 'petal length': 1.1, 'petal width': 0.9}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    response = requests.get(\"http://localhost:8000/regressor\", json=sample_request_input).json()\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployments: {'BoostingModel': Deployment(name=BoostingModel,version=None,route_prefix=/regressor)}\n"
     ]
    }
   ],
   "source": [
    "deployments = serve.list_deployments()\n",
    "print(f'deployments: {deployments}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=63313)\u001b[0m 2022-03-16 16:35:06,116\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'BoostingModel'. component=serve deployment=BoostingModel\n"
     ]
    }
   ],
   "source": [
    "serve.shutdown() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Try Adding more examples\n",
    "\n",
    "Here are some things you can try:\n",
    "\n",
    "1. Send more input requests.\n",
    "2. Add a small model of your own"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
