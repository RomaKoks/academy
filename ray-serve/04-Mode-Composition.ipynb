{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a05911a8-854d-400e-9437-23fe97969978",
   "metadata": {},
   "source": [
    "# Ray Serve - Model Composition\n",
    "\n",
    "© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../images/AnyscaleAcademyLogo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06def7c0-7404-4d42-bba5-bd34895e6ee6",
   "metadata": {},
   "source": [
    "Ray Serve supports composing individually scalable models into a single model out of the box. For instance, you can combine multiple models to perform stacking or ensembles.\n",
    "\n",
    "To define a higher-level composed model you need to do three things:\n",
    "\n",
    " 1. Define your underlying models (the ones that you will compose together) as Ray Serve deployments.\n",
    "\n",
    " 2. Define your composed model, using the handles of the underlying models \n",
    "\n",
    " 3. Define a deployment representing this composed model and query it!\n",
    "\n",
    "In order to avoid synchronous execution in the composed model (e.g., it’s very slow to make calls to the composed model), you’ll need to make the function asynchronous by using an `async` def. \n",
    "\n",
    "Our pipeline will be structured as follows:\n",
    " * Input comes in, the composed model sends it to model_one\n",
    " * model_one outputs a random number between 0 and 1, if the value is\n",
    " * greater than 0.5, then the data is sent to model_two\n",
    " * otherwise, the data is returned to the user.\n",
    "\n",
    "Let's define two models that just print out the data they received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23459658-0022-41ab-939d-336b4b61df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "import requests\n",
    "import ray\n",
    "from ray import serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69afa52-e578-4ec6-8efb-a2f9292e38c6",
   "metadata": {},
   "source": [
    "We are using stateless functions as our deployments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4443a6e4-6e61-4a5e-8c1b-a3644d783a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "def model_one(data):\n",
    "    print(f\"Model 1 called with data: {data}\")\n",
    "    return random()\n",
    "\n",
    "@serve.deployment\n",
    "def model_two(data):\n",
    "    print(f\"Model 2 called with data: {data}\")\n",
    "    # Use this data sent from model_one\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a180ea0-6009-4d8e-8858-415f8341f96b",
   "metadata": {},
   "source": [
    "`max_concurrent_queries` is optional. By default, if you pass in an async\n",
    "function, Ray Serve sets the limit to a high number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "879dcb0a-d309-4eea-8846-d1f052557736",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(max_concurrent_queries=10, route_prefix='/composed')\n",
    "class ComposedModel:\n",
    "    def __init__(self):\n",
    "        # Use the Python ServeHandle APIs\n",
    "        # Set sync=False to override default, which is use this in a synchronous mode.\n",
    "        # We want these deployments to be run within a asynchronous event loop for concurrency\n",
    "        # See documentation for Sync Async ServeHandle APIs for details:\n",
    "        # https://docs.ray.io/en/master/serve/http-servehandle.html#sync-and-async-handles\n",
    "        \n",
    "        self.model_one = model_one.get_handle(sync=False)\n",
    "        self.model_two = model_two.get_handle(sync=False)\n",
    "\n",
    "    # This method can be called concurrently\n",
    "    async def __call__(self, starlette_request):\n",
    "        # at this point you are yielding to the event loop take in another request\n",
    "        data = await starlette_request.body()\n",
    "\n",
    "        # Use await twice here for two reasons:\n",
    "        # 1. Since we are running within a async def callable function and we want to use\n",
    "        # this model_one deployment to run in an asynchronous fashion, this is standard\n",
    "        # async-await pattern. This await call will return an ObjectRef.\n",
    "        # 2. The second await waits on the ObjectRef to do an implicit ray.get(Object) to\n",
    "        # fetch the actual value returned.\n",
    "        # Hence two awaits.\n",
    "        score = await(await self.model_one.remote(data=data))\n",
    "        if score > 0.5:\n",
    "            await (await self.model_two.remote(data=data))\n",
    "            result = {\"model_used: 1 & 2;  score\": score}\n",
    "        else:\n",
    "            result = {\"model_used: 1 ; score\": score}\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb07519b-f04d-47a8-b071-6a981c561f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-19 02:06:41,915\tINFO services.py:1338 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '127.0.0.1',\n",
       " 'raylet_ip_address': '127.0.0.1',\n",
       " 'redis_address': '127.0.0.1:46607',\n",
       " 'object_store_address': '/tmp/ray/session_2022-02-19_02-06-38_862887_19079/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-02-19_02-06-38_862887_19079/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8267',\n",
       " 'session_dir': '/tmp/ray/session_2022-02-19_02-06-38_862887_19079',\n",
       " 'metrics_export_port': 65470,\n",
       " 'node_id': '5cae9aed97eba29607439fa4144cc2f61d902adab9f2758c3f6e151e'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start ray with 8 processes\n",
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "ray.init(num_cpus=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcabfe9d-a17f-4d42-ad26-607652e65108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=19980)\u001b[0m 2022-02-19 02:07:14,107\tINFO checkpoint_path.py:16 -- Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(ServeController pid=19980)\u001b[0m 2022-02-19 02:07:14,111\tINFO http_state.py:98 -- Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:JfJOso:SERVE_PROXY_ACTOR-node:127.0.0.1-0' on node 'node:127.0.0.1-0' listening on '127.0.0.1:8000'\n",
      "2022-02-19 02:07:14,505\tINFO api.py:463 -- Started Serve instance in namespace 'bb3eb743-f34c-4f7a-8a06-e6ceb1a3471d'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.serve.api.Client at 0x7fa210182370>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=19977)\u001b[0m INFO:     Started server process [19977]\n"
     ]
    }
   ],
   "source": [
    "serve.start()    # will start a serve instance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c90c8-46fc-4148-bf7b-b90902a91504",
   "metadata": {},
   "source": [
    "### Start deployment instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cfe4dd6-78dc-4f7d-b98e-193f69ef26f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-19 02:08:06,395\tINFO api.py:242 -- Updating deployment 'model_one'. component=serve deployment=model_one\n",
      "\u001b[2m\u001b[36m(ServeController pid=19980)\u001b[0m 2022-02-19 02:08:06,433\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'model_one'. component=serve deployment=model_one\n",
      "2022-02-19 02:08:06,881\tINFO api.py:249 -- Deployment 'model_one' is ready at `http://127.0.0.1:8000/model_one`. component=serve deployment=model_one\n",
      "2022-02-19 02:08:06,886\tINFO api.py:242 -- Updating deployment 'model_two'. component=serve deployment=model_two\n",
      "\u001b[2m\u001b[36m(ServeController pid=19980)\u001b[0m 2022-02-19 02:08:06,989\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'model_two'. component=serve deployment=model_two\n",
      "2022-02-19 02:08:07,433\tINFO api.py:249 -- Deployment 'model_two' is ready at `http://127.0.0.1:8000/model_two`. component=serve deployment=model_two\n",
      "2022-02-19 02:08:07,439\tINFO api.py:242 -- Updating deployment 'ComposedModel'. component=serve deployment=ComposedModel\n",
      "\u001b[2m\u001b[36m(ServeController pid=19980)\u001b[0m 2022-02-19 02:08:07,537\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'ComposedModel'. component=serve deployment=ComposedModel\n",
      "2022-02-19 02:08:07,983\tINFO api.py:249 -- Deployment 'ComposedModel' is ready at `http://127.0.0.1:8000/composed`. component=serve deployment=ComposedModel\n"
     ]
    }
   ],
   "source": [
    "model_one.deploy()\n",
    "model_two.deploy()\n",
    "ComposedModel.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78d04a9c-c684-4263-87d6-505c95d604b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_one': Deployment(name=model_one,version=None,route_prefix=/model_one),\n",
       " 'model_two': Deployment(name=model_two,version=None,route_prefix=/model_two),\n",
       " 'ComposedModel': Deployment(name=ComposedModel,version=None,route_prefix=/composed)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve.list_deployments()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cf40f5-29e9-4332-ac6a-bb7bbb270ed2",
   "metadata": {},
   "source": [
    "#### Send requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df6334c1-fd2b-427b-8526-e972e62fee99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_used: 1 ; score': 0.4446714648125224}\n",
      "{'model_used: 1 ; score': 0.3584252167752343}\n",
      "{'model_used: 1 & 2;  score': 0.9944325362625361}\n",
      "{'model_used: 1 ; score': 0.17709078567333958}\n",
      "{'model_used: 1 ; score': 0.43465235796229407}\n",
      "{'model_used: 1 & 2;  score': 0.8347432503521646}\n",
      "{'model_used: 1 ; score': 0.2922063099356801}\n",
      "{'model_used: 1 & 2;  score': 0.8178159182653494}\n",
      "\u001b[2m\u001b[36m(model_one pid=19976)\u001b[0m Model 1 called with data:b'Hey!'\n",
      "\u001b[2m\u001b[36m(model_one pid=19976)\u001b[0m Model 1 called with data:b'Hey!'\n",
      "\u001b[2m\u001b[36m(model_one pid=19976)\u001b[0m Model 1 called with data:b'Hey!'\n",
      "\u001b[2m\u001b[36m(model_one pid=19976)\u001b[0m Model 1 called with data:b'Hey!'\n",
      "\u001b[2m\u001b[36m(model_one pid=19976)\u001b[0m Model 1 called with data:b'Hey!'\n",
      "\u001b[2m\u001b[36m(model_one pid=19976)\u001b[0m Model 1 called with data:b'Hey!'\n",
      "\u001b[2m\u001b[36m(model_one pid=19976)\u001b[0m Model 1 called with data:b'Hey!'\n",
      "\u001b[2m\u001b[36m(model_one pid=19976)\u001b[0m Model 1 called with data:b'Hey!'\n",
      "\u001b[2m\u001b[36m(model_two pid=19979)\u001b[0m Model 2 called with data:b'Hey!'\n",
      "\u001b[2m\u001b[36m(model_two pid=19979)\u001b[0m Model 2 called with data:b'Hey!'\n",
      "\u001b[2m\u001b[36m(model_two pid=19979)\u001b[0m Model 2 called with data:b'Hey!'\n"
     ]
    }
   ],
   "source": [
    "for _ in range(8):\n",
    "    resp = requests.get(\"http://127.0.0.1:8000/composed\", data=\"Hey!\")\n",
    "    print(resp.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7b147cc-397f-48df-a12d-7af958cfdaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
