{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Train - Overview\n",
    "\n",
    "Â© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../images/AnyscaleAcademyLogo.png)\n",
    "\n",
    "## Watch Ray Summit 2021 On demand!\n",
    "\n",
    "Join us for the [_free_ Ray Summit 2021 virtual conference](https://www.anyscale.com/ray-summit-2021) on June 22-24, 2021. We have an amazing lineup of luminar keynote speakers and breakout sessions on the Ray ecosystem, third-party Ray libraries, and applications of Ray in the real world.\n",
    "\n",
    "\n",
    "## About This Tutorial\n",
    "<img src=\"https://images.ctfassets.net/xjan103pcp94/7kS5Mc1LeZRPqVlItSGMNI/3b863c02e9d0569da038c69ec2537864/image_1.jpg\"  width=\"800\" height=\"200\" alt=\"Ray Summit 2021\"/>\n",
    "</a>\n",
    "\n",
    "[Ray Train](https://docs.ray.io/en/latest/train/train.html), formerly known as **Ray SGD**, is a lightweight library for distributed deep learning, allowing you to scale up and speed up training for your deep learning models. Currently, Ray Train is available as beta (or experimental) in Ray 1.9 release, offering the following features:\n",
    "\n",
    " * Scale to multi-GPU and multi-node training with 0 code changes\n",
    "\n",
    " * Runs seamlessly on any cloud (AWS, GCP, Azure, Kubernetes, or on-prem)\n",
    "\n",
    " * Supports PyTorch, TensorFlow, and Horovod \n",
    "\n",
    " * Distributed data loading and hyperparameter tuning\n",
    "\n",
    " * Built-in loggers for TensorBoard and MLflow\n",
    "\n",
    "See the instructions in the [README](../README.md) for setting up your environment to use this tutorial.\n",
    "\n",
    "Go [here](../Overview.ipynb) for an overview of all tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|     | Lesson | Description |\n",
    "| :-- | :----- | :---------- |\n",
    "| 00  | [Ray Train Overview](00-Ray-Train-Overview.ipynb) | Overview of this tutorial. |\n",
    "| 01  | [Ray Train Quickstart ](01-Ray-Train-Quickstart.ipynb) | A quick start on PyTorch training on single worker. |\n",
    "| 02  | [Ray Train Quickstart Distributed](02-Ray-Train-Quickstart-Distributed.ipynb) |A quick start on PyTorch Distributed training on multiple workers . |\n",
    "| 03  | [Ray Train with PyTorch](03-Ray-Train-with-PyTorch.ipynb) | Use Ray Train Distributed API to train a linear model |\n",
    "| 04  | [Ray Train with PyTorch and FashionMNSIT](04-Ray-Train-with-PyTorch-FashionMNIST.ipynb) | Use Ray Train Distributed API to train a NN for FashionMNIST |\n",
    "|     | [Ray Train Examples](https://docs.ray.io/en/latest/train/examples.html) | Explore examples for Ray Train with PyTorch, TensorFlow, and Horvod. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Help\n",
    "\n",
    "* The [#tutorial channel](https://ray-distributed.slack.com/archives/C011ML23W5B) on the [Ray Slack](https://ray-distributed.slack.com). [Click here](https://forms.gle/9TSdDYUgxYs8SA9e8) to join.\n",
    "* [Email](mailto:academy@anyscale.com)\n",
    "\n",
    "Find an issue? Please report it!\n",
    "\n",
    "* [GitHub issues](https://github.com/anyscale/academy/issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give Us Feedback!\n",
    "\n",
    "Let us know what you like and don't like about this HPO and Ray Tune tutorial.\n",
    "\n",
    "* [Survey](https://forms.gle/StzNufFyyDT3dapt8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
