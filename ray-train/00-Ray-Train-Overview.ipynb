{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Train - Overview\n",
    "\n",
    "Â© 2019-2021, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../images/AnyscaleAcademyLogo.png)\n",
    "\n",
    "## Watch Sessions from Ray Summit 2021!\n",
    "\n",
    "We had an amazing lineup of luminar keynote speakers and breakout sessions on the Ray ecosystem, third-party Ray libraries, and applications of Ray in the real world.\n",
    "\n",
    "<a href=\"https://www.anyscale.com/ray-summit-2021\">\n",
    "<img src=\"../images/raysummit-horizontal-white-banner-full.png\"  width=\"800\" height=\"200\" alt=\"Ray Summit 2021\"/>\n",
    "</a>\n",
    "\n",
    "## About This Tutorial\n",
    "\n",
    "![Ray Train](../images/RaySGD.png)\n",
    "\n",
    "Ray Train, formerly known as Ray SGD, is a lightweight library for distributed deep learning, allowing you to scale up and speed up training for your deep learning models. Currently available as beta or experimental in Ray 1.9 release.\n",
    "\n",
    "The main features are:\n",
    "\n",
    " **Ease of use**: Scale your single process training code to a cluster in just a couple lines of code.\n",
    " \n",
    " **Composability**: Ray Train interoperates with [Ray Tune](https://docs.ray.io/en/latest/tune/index.html#tune-main) to tune your distributed model and [Ray Datasets](https://docs.ray.io/en/latest/data/dataset.html#datasets) to train on large amounts of data.\n",
    " \n",
    " **Interactivity**: Ray Train fits in your workflow with support to run from any environment, including seamless Jupyter notebook support.\n",
    "\n",
    "See the instructions in the [README](../README.md) for setting up your environment to use this tutorial.\n",
    "\n",
    "Go [here](../Overview.ipynb) for an overview of all tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|     | Lesson | Description |\n",
    "| :-- | :----- | :---------- |\n",
    "| 00  | [Ray Train Overview](00-Ray-Train-Overview.ipynb) | Overview of this tutorial. |\n",
    "| 01  | [Ray Train Quickstart ](01-Ray-Train-Quickstart.ipynb) | A quick start on PyTorch training on single worker. |\n",
    "| 02  | [Ray Train Quickstart Distributed](02-Ray-Train-Quickstart-Distributed.ipynb) |A quick start on PyTorch Distributed training on multiple workers . |\n",
    "| 03  | [Ray Train with PyTorch](03-Ray-Train-with-PyTorch.ipynb) | Use Ray Train Distributed API to train a linear model) |\n",
    "|     | [Ray Train Examples](https://docs.ray.io/en/latest/train/examples.html) | Explore examples for Ray Train with PyTorch, TensorFlow, and Horvod. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Help\n",
    "\n",
    "* The [#tutorial channel](https://ray-distributed.slack.com/archives/C011ML23W5B) on the [Ray Slack](https://ray-distributed.slack.com). [Click here](https://forms.gle/9TSdDYUgxYs8SA9e8) to join.\n",
    "* [Email](mailto:academy@anyscale.com)\n",
    "\n",
    "Find an issue? Please report it!\n",
    "\n",
    "* [GitHub issues](https://github.com/anyscale/academy/issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give Us Feedback!\n",
    "\n",
    "Let us know what you like and don't like about this HPO and Ray Tune tutorial.\n",
    "\n",
    "* [Survey](https://forms.gle/StzNufFyyDT3dapt8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
